{
  "hash": "cc7b03f1d084b0ecb095220b59035953",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Face Recognition using Convolutional Neural Networks\ndescription: Building a CNN model for facial recognition using the ORL database.\nauthor: Chaance Graves\ndate: \"2023-11-21\" #  November 21, 2023\ncategories: [deep learning, computer vision, CNN]\n#image: none\njupyter: python3\n\nformat: \n  html:\n    toc: true\n    toc-title: Contents\n    toc-location: right\n    code-fold: false\n---\n\n\n\n\n<a href=\"https://colab.research.google.com/github/ctg123/ml-projects/blob/main/facial-recognition-cnn/face_recognition_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Project Description\n\nFacial recognition technology has numerous applications in security, authentication, and personalization systems. This project implements a Convolutional Neural Network (CNN) to accurately identify individuals from facial images using the ORL database of faces.\n\n## Import the relevant packages and necessary dependencies\n\n::: {#e9ae4c42 .cell tags='[]' execution_count=1}\n``` {.python .cell-code}\n# Python built-in libraries\nfrom pathlib import Path\n\n# Data pre-processing and visualization\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sci-kit learn functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# For model training and compilation\nfrom keras import layers\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\n\n# suppress warnings output messages\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2025-04-24 08:40:04.323420: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-04-24 08:40:04.325411: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-04-24 08:40:04.329396: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-04-24 08:40:04.338178: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745502004.352729   13156 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745502004.357495   13156 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1745502004.377104   13156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1745502004.377134   13156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1745502004.377136   13156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1745502004.377137   13156 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-04-24 08:40:04.384672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\n## Upload and import the data\n\nLet's load and normalize the images\n\n::: {#a079b729 .cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nimage_dir = Path('datasets/ORL_face_database')\n\nimage_dir\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nPosixPath('datasets/ORL_face_database')\n```\n:::\n:::\n\n\n::: {#70aa9648 .cell tags='[]' execution_count=3}\n``` {.python .cell-code}\n# Create empty arrays to store the training and test images and labels:\nimages = []\nlabels = []\n```\n:::\n\n\n::: {#72d67f30 .cell tags='[]' execution_count=4}\n``` {.python .cell-code}\n# Iterate over the subdirectories of the dataset (representing different classes or labels)\nfor person_dir in image_dir.iterdir():\n    if person_dir.is_dir():\n        label = int(person_dir.name[1:])\n\n        for image_file in person_dir.iterdir():\n            image = cv2.imread(str(image_file))\n            grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            numpy_image = np.array(grayscale_image)\n\n            # Append images and labels directly to the trainX and trainY lists\n            images.append(numpy_image)\n            labels.append(label)\n```\n:::\n\n\n::: {#100e68dd .cell tags='[]' execution_count=5}\n``` {.python .cell-code}\n# Convert the image data and labels into NumPy arrays\nimages = np.array(images)\nimages = images.astype(\"float32\") / 255  # Normalize the images\nlabels = np.array(labels)\n```\n:::\n\n\n::: {#cfda4e4d .cell tags='[]' execution_count=6}\n``` {.python .cell-code}\nprint(f'normalized image data: {images[:]}')\nprint('\\n')\n# Check the shapes after modifications\nprint(f\"numpy images shape: {images.shape}\")\nprint(f\"labels: {labels.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnormalized image data: [[[0.3372549  0.3529412  0.34117648 ... 0.32941177 0.33333334 0.34117648]\n  [0.3647059  0.3372549  0.34117648 ... 0.34509805 0.32941177 0.34509805]\n  [0.34901962 0.3372549  0.34509805 ... 0.34509805 0.32941177 0.34117648]\n  ...\n  [0.38039216 0.40392157 0.49411765 ... 0.42745098 0.36862746 0.31764707]\n  [0.69411767 0.7137255  0.7490196  ... 0.5294118  0.49803922 0.52156866]\n  [0.79607844 0.79607844 0.79607844 ... 0.6        0.5568628  0.54509807]]\n\n [[0.37254903 0.34901962 0.34901962 ... 0.40392157 0.4392157  0.35686275]\n  [0.37254903 0.37254903 0.3647059  ... 0.34901962 0.41960785 0.43529412]\n  [0.35686275 0.37254903 0.36862746 ... 0.3137255  0.3882353  0.39215687]\n  ...\n  [0.6784314  0.81960785 0.8156863  ... 0.22352941 0.21176471 0.21176471]\n  [0.7058824  0.81960785 0.81960785 ... 0.21176471 0.22745098 0.21176471]\n  [0.72156864 0.8117647  0.8156863  ... 0.16862746 0.21568628 0.21568628]]\n\n [[0.35686275 0.3254902  0.27450982 ... 0.63529414 0.6313726  0.6156863 ]\n  [0.32156864 0.31764707 0.28235295 ... 0.5803922  0.64705884 0.63529414]\n  [0.3372549  0.2901961  0.27450982 ... 0.5411765  0.6431373  0.65882355]\n  ...\n  [0.21176471 0.19607843 0.20392157 ... 0.6117647  0.70980394 0.75686276]\n  [0.20392157 0.2        0.2        ... 0.64705884 0.7294118  0.7647059 ]\n  [0.1882353  0.21960784 0.1764706  ... 0.67058825 0.7372549  0.74509805]]\n\n ...\n\n [[0.52156866 0.5058824  0.52156866 ... 0.49411765 0.49411765 0.4862745 ]\n  [0.5058824  0.5176471  0.5137255  ... 0.49019608 0.5019608  0.49411765]\n  [0.5176471  0.5058824  0.5137255  ... 0.49411765 0.49019608 0.49803922]\n  ...\n  [0.11764706 0.12156863 0.07843138 ... 0.08627451 0.12941177 0.1254902 ]\n  [0.11764706 0.10196079 0.09019608 ... 0.08235294 0.11372549 0.12156863]\n  [0.09019608 0.11764706 0.08235294 ... 0.05882353 0.11372549 0.10196079]]\n\n [[0.53333336 0.53333336 0.53333336 ... 0.5019608  0.52156866 0.52156866]\n  [0.5372549  0.5372549  0.5411765  ... 0.50980395 0.52156866 0.49803922]\n  [0.5254902  0.54901963 0.5372549  ... 0.5254902  0.52156866 0.5019608 ]\n  ...\n  [0.23137255 0.23529412 0.21568628 ... 0.0627451  0.08627451 0.08627451]\n  [0.25490198 0.23921569 0.1882353  ... 0.06666667 0.08627451 0.07450981]\n  [0.23529412 0.1882353  0.21960784 ... 0.07450981 0.07843138 0.08235294]]\n\n [[0.5254902  0.5411765  0.53333336 ... 0.5058824  0.5137255  0.49803922]\n  [0.5254902  0.5294118  0.5294118  ... 0.50980395 0.50980395 0.5058824 ]\n  [0.52156866 0.5254902  0.5294118  ... 0.49803922 0.5176471  0.52156866]\n  ...\n  [0.09803922 0.07450981 0.08235294 ... 0.10588235 0.22352941 0.31764707]\n  [0.10196079 0.08627451 0.07450981 ... 0.11764706 0.2784314  0.31764707]\n  [0.08235294 0.09803922 0.08627451 ... 0.21568628 0.30980393 0.31764707]]]\n\n\nnumpy images shape: (400, 112, 92)\nlabels: (400,)\n```\n:::\n:::\n\n\n## Let's visualize the Images and Train and Test Dataset\n\n::: {#9b495e8d .cell tags='[]' execution_count=7}\n``` {.python .cell-code}\n# Define the number of images you want to plot\nnum_images_to_plot = 25  # Change this number as needed\n\n# Reshape the images to (112, 92)\n# Assuming x_train has shape (num_samples, 112, 92)\nreshaped_images = images[:num_images_to_plot].reshape(-1, 112, 92)\n\n# Plot the images\nplt.figure(figsize=(12, 12))\nfor i in range(num_images_to_plot):\n    plt.subplot(5, 5, i + 1)  # Change the subplot layout as per your preference\n    plt.imshow(reshaped_images[i], cmap='gray')\n    # Set the title as per the corresponding label\n    plt.title(f'Subject Person: {labels[i]}\\n ({reshaped_images[i].shape[1]}, {reshaped_images[i].shape[0]})')\n    #plt.axis('off')  # Hide axes\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n## Split the Data into Train, Test and Validation Sets\n\n::: {#8197a592 .cell tags='[]' execution_count=8}\n``` {.python .cell-code}\nX_data = images # store images in X_data\nY_data = labels.reshape(-1,1) # store labels in Y_data\n\n# Find unique classes in the labels\nunique_labels = np.unique(Y_data)\n\nunique_labels\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40])\n```\n:::\n:::\n\n\n::: {#92bc5fe5 .cell tags='[]' execution_count=9}\n``` {.python .cell-code}\n# Reindex the labels to start from 0\nlabel_mapping = {label: index for index, label in enumerate(unique_labels)}\nY_data_reindexed = np.array([label_mapping[label[0]] for label in Y_data])\n\n# Verify the unique values in the reindexed labels\nprint(np.unique(Y_data_reindexed))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]\n```\n:::\n:::\n\n\n1. Initial Training and Test Split:\n    - Initially, you've split the data into x_train (training data) and x_test (test data).\n2. Create Validation Set:\n    - To generate a validation set from the existing training data (x_train), you can perform another split. This split will create a subset designated for validation purposes.\n\n::: {#e6711978 .cell tags='[]' execution_count=10}\n``` {.python .cell-code}\n# take a random sample: 80% of the data for the test set\n\n# The resulting variables will represent:\n# x_train: Training data\n# y_train: Corresponding training labels\n# x_test: Test data\n# y_test: Corresponding test labels\n\nx_train, x_test, y_train, y_test = train_test_split(X_data, Y_data_reindexed, test_size=0.2, random_state=42)\n\nprint(f'x_train: {x_train.shape}')\nprint(f'x_test: {x_test.shape}')\nprint(f'y_train: {y_train.shape}')\nprint(f'y_test: {y_test.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx_train: (320, 112, 92)\nx_test: (80, 112, 92)\ny_train: (320,)\ny_test: (80,)\n```\n:::\n:::\n\n\n::: {#27f88098 .cell tags='[]' execution_count=11}\n``` {.python .cell-code}\n# Split the training data further into x_train, x_val, y_train, y_val\n\n# The resulting variables will represent:\n# x_val: Validation data\n# y_val: Corresponding validation labels\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.05, random_state=42)\n\nprint(f'x_train: {x_train.shape}')\nprint(f'x_val: {x_val.shape}')\nprint(f'y_train: {y_train.shape}')\nprint(f'y_val: {y_val.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx_train: (304, 112, 92)\nx_val: (16, 112, 92)\ny_train: (304,)\ny_val: (16,)\n```\n:::\n:::\n\n\n* `x_train`, `y_train`: These represent the primary training dataset and labels, comprising 60% of the original data (80% of 80%).\n* `x_test`, `y_test`: These represent the test dataset and labels, comprising 20% of the original data.\n* `x_val`, `y_val`: These represent the validation dataset and labels, comprising 5% of the original dataset.\n\n### Check the images are equal sizes to prepare the data for input to the CNN model\n\n::: {#128805a3 .cell tags='[]' execution_count=12}\n``` {.python .cell-code}\n# Assuming x_train and x_val contain the image data\n# Reshape the input data to match the expected input shape\nx_train = x_train.reshape(-1, 112, 92, 1)\nx_test = x_test.reshape(-1, 112, 92, 1)\nx_val = x_val.reshape(-1, 112, 92, 1)\ninput_shape = (x_train.shape[1], x_train.shape[2], 1)\n\nprint(f'Input Shape: {input_shape}')\nprint(f'x_train: {x_train.shape}')\nprint(f'x_test: {x_test.shape}')\nprint(f'x_val: {x_val.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInput Shape: (112, 92, 1)\nx_train: (304, 112, 92, 1)\nx_test: (80, 112, 92, 1)\nx_val: (16, 112, 92, 1)\n```\n:::\n:::\n\n\n::: {#f4bba785 .cell tags='[]' execution_count=13}\n``` {.python .cell-code}\n# Convert labels to categorical format\nnum_classes = len(np.unique(Y_data_reindexed))\n\n# Convert integer labels to one-hot encoded labels\ny_train_categorical = to_categorical(y_train, num_classes)\ny_test_categorical = to_categorical(y_test, num_classes)\ny_val_categorical = to_categorical(y_val, num_classes)\n\nprint(f'The number of the classes: {num_classes}')\nprint(f'y_train categorical shape: {y_train_categorical.shape}')\nprint(f'y_test categorical shape: {y_test_categorical.shape}')\nprint(f'y_val categorical shape: {y_val_categorical.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe number of the classes: 40\ny_train categorical shape: (304, 40)\ny_test categorical shape: (80, 40)\ny_val categorical shape: (16, 40)\n```\n:::\n:::\n\n\n## Build the CNN Model\n\n1. Convolotional layer\n2. pooling layer\n3. fully connected layer\n\nLet's build a new architecture of CNN by changing the number and position of layers.\n\n::: {#7f782c7c .cell tags='[]' execution_count=14}\n``` {.python .cell-code}\n# Adding the hidden layers and the output layer to our model\ncnn_model = Sequential([\nlayers.Conv2D(32, (3, 3), activation='relu', input_shape= input_shape),\nlayers.BatchNormalization(),\nlayers.MaxPooling2D((2, 2)),\n#layers.BatchNormalization(),\nlayers.Conv2D(64, (3, 3), activation='relu', input_shape= input_shape), # Additional Conv2D layer\n#layers.BatchNormalization(),\nlayers.MaxPooling2D((2, 2)),\n\n# Fully Connected\nlayers.Flatten(),\n\nlayers.Dense(256, activation='relu'),\n# Dense layers with Dropout\nlayers.Dropout(0.5),\nlayers.Dense(128, activation='relu'),\n#layers.Dropout(0.5),\nlayers.Dense(num_classes, activation='softmax')\n])\n\n# Display the summary of the model architecture and the number of parameters\ncnn_model.summary()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2025-04-24 08:40:08.129008: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34944</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,945,920</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,160</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,002,920</span> (34.34 MB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,002,856</span> (34.34 MB)\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> (256.00 B)\n</pre>\n```\n:::\n:::\n\n\n::: {#98810b22 .cell tags='[]' execution_count=15}\n``` {.python .cell-code}\n# Compile the model\ncnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n```\n:::\n\n\n## Train the Model\n\nTrain the model with `100` epochs and we'll plot training loss and accuracy against epochs. We want to monitor the validation loss at each epoch and after the validation loss has not improved after `10` epochs, training is interrupted.\n\n::: {#75989572 .cell tags='[]' execution_count=16}\n``` {.python .cell-code}\n# Define the early stopping callback\nearly_stopping = EarlyStopping(monitor='val_loss',\n                               patience=10,\n                               restore_best_weights=True)\n```\n:::\n\n\n::: {#0152d9fd .cell tags='[]' execution_count=17}\n``` {.python .cell-code}\nhistory = cnn_model.fit(np.array(x_train), y_train_categorical,\n                        epochs=100,\n                        verbose=2,\n                        validation_data=(np.array(x_val), y_val_categorical),\n                        callbacks=[early_stopping])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/100\n10/10 - 3s - 267ms/step - accuracy: 0.0230 - loss: 5.9299 - val_accuracy: 0.1250 - val_loss: 3.6867\nEpoch 2/100\n10/10 - 1s - 111ms/step - accuracy: 0.0987 - loss: 3.5003 - val_accuracy: 0.0000e+00 - val_loss: 3.6894\nEpoch 3/100\n10/10 - 1s - 111ms/step - accuracy: 0.1809 - loss: 3.1518 - val_accuracy: 0.0000e+00 - val_loss: 3.6889\nEpoch 4/100\n10/10 - 1s - 113ms/step - accuracy: 0.3520 - loss: 2.6107 - val_accuracy: 0.0000e+00 - val_loss: 3.6785\nEpoch 5/100\n10/10 - 4s - 417ms/step - accuracy: 0.4112 - loss: 2.1143 - val_accuracy: 0.0625 - val_loss: 3.6438\nEpoch 6/100\n10/10 - 1s - 113ms/step - accuracy: 0.5329 - loss: 1.6213 - val_accuracy: 0.0625 - val_loss: 3.6190\nEpoch 7/100\n10/10 - 1s - 111ms/step - accuracy: 0.7039 - loss: 1.0945 - val_accuracy: 0.0625 - val_loss: 3.6088\nEpoch 8/100\n10/10 - 1s - 115ms/step - accuracy: 0.6776 - loss: 1.0109 - val_accuracy: 0.0625 - val_loss: 3.5772\nEpoch 9/100\n10/10 - 1s - 114ms/step - accuracy: 0.7730 - loss: 0.7799 - val_accuracy: 0.0000e+00 - val_loss: 3.5266\nEpoch 10/100\n10/10 - 1s - 113ms/step - accuracy: 0.7434 - loss: 0.7901 - val_accuracy: 0.1875 - val_loss: 3.5233\nEpoch 11/100\n10/10 - 1s - 111ms/step - accuracy: 0.8783 - loss: 0.4624 - val_accuracy: 0.1250 - val_loss: 3.4865\nEpoch 12/100\n10/10 - 1s - 109ms/step - accuracy: 0.8586 - loss: 0.4446 - val_accuracy: 0.3125 - val_loss: 3.4084\nEpoch 13/100\n10/10 - 1s - 109ms/step - accuracy: 0.8520 - loss: 0.4221 - val_accuracy: 0.2500 - val_loss: 3.4584\nEpoch 14/100\n10/10 - 1s - 111ms/step - accuracy: 0.8783 - loss: 0.4136 - val_accuracy: 0.3750 - val_loss: 3.4547\nEpoch 15/100\n10/10 - 1s - 108ms/step - accuracy: 0.8914 - loss: 0.3152 - val_accuracy: 0.3125 - val_loss: 3.4135\nEpoch 16/100\n10/10 - 1s - 111ms/step - accuracy: 0.9079 - loss: 0.3323 - val_accuracy: 0.5000 - val_loss: 3.3642\nEpoch 17/100\n10/10 - 1s - 109ms/step - accuracy: 0.9243 - loss: 0.2643 - val_accuracy: 0.5000 - val_loss: 3.3989\nEpoch 18/100\n10/10 - 1s - 114ms/step - accuracy: 0.8980 - loss: 0.3166 - val_accuracy: 0.6250 - val_loss: 3.3122\nEpoch 19/100\n10/10 - 1s - 110ms/step - accuracy: 0.9145 - loss: 0.2861 - val_accuracy: 0.7500 - val_loss: 3.2894\nEpoch 20/100\n10/10 - 1s - 110ms/step - accuracy: 0.9342 - loss: 0.2391 - val_accuracy: 0.8125 - val_loss: 3.2451\nEpoch 21/100\n10/10 - 1s - 109ms/step - accuracy: 0.9276 - loss: 0.2563 - val_accuracy: 0.7500 - val_loss: 3.2335\nEpoch 22/100\n10/10 - 1s - 112ms/step - accuracy: 0.9243 - loss: 0.2438 - val_accuracy: 0.6875 - val_loss: 3.2463\nEpoch 23/100\n10/10 - 1s - 115ms/step - accuracy: 0.9441 - loss: 0.2171 - val_accuracy: 0.6875 - val_loss: 3.1442\nEpoch 24/100\n10/10 - 1s - 111ms/step - accuracy: 0.9276 - loss: 0.2010 - val_accuracy: 0.8125 - val_loss: 2.9902\nEpoch 25/100\n10/10 - 1s - 109ms/step - accuracy: 0.9605 - loss: 0.1603 - val_accuracy: 0.8750 - val_loss: 2.7800\nEpoch 26/100\n10/10 - 1s - 110ms/step - accuracy: 0.9572 - loss: 0.1312 - val_accuracy: 0.7500 - val_loss: 2.7827\nEpoch 27/100\n10/10 - 1s - 110ms/step - accuracy: 0.9638 - loss: 0.1356 - val_accuracy: 0.8125 - val_loss: 2.7697\nEpoch 28/100\n10/10 - 1s - 112ms/step - accuracy: 0.9474 - loss: 0.1750 - val_accuracy: 0.8125 - val_loss: 2.6994\nEpoch 29/100\n10/10 - 1s - 110ms/step - accuracy: 0.9474 - loss: 0.1363 - val_accuracy: 0.9375 - val_loss: 2.6153\nEpoch 30/100\n10/10 - 1s - 112ms/step - accuracy: 0.9276 - loss: 0.1971 - val_accuracy: 0.8125 - val_loss: 2.3204\nEpoch 31/100\n10/10 - 1s - 116ms/step - accuracy: 0.9375 - loss: 0.2138 - val_accuracy: 0.8125 - val_loss: 2.2279\nEpoch 32/100\n10/10 - 4s - 423ms/step - accuracy: 0.9408 - loss: 0.1866 - val_accuracy: 1.0000 - val_loss: 2.0679\nEpoch 33/100\n10/10 - 1s - 116ms/step - accuracy: 0.9474 - loss: 0.1778 - val_accuracy: 0.8750 - val_loss: 2.2333\nEpoch 34/100\n10/10 - 1s - 111ms/step - accuracy: 0.9539 - loss: 0.1598 - val_accuracy: 1.0000 - val_loss: 2.3613\nEpoch 35/100\n10/10 - 1s - 113ms/step - accuracy: 0.9605 - loss: 0.1347 - val_accuracy: 1.0000 - val_loss: 2.1150\nEpoch 36/100\n10/10 - 1s - 112ms/step - accuracy: 0.9375 - loss: 0.1855 - val_accuracy: 0.9375 - val_loss: 1.9878\nEpoch 37/100\n10/10 - 1s - 114ms/step - accuracy: 0.9342 - loss: 0.2047 - val_accuracy: 1.0000 - val_loss: 1.7495\nEpoch 38/100\n10/10 - 1s - 113ms/step - accuracy: 0.9441 - loss: 0.1502 - val_accuracy: 1.0000 - val_loss: 1.2873\nEpoch 39/100\n10/10 - 1s - 122ms/step - accuracy: 0.9309 - loss: 0.1838 - val_accuracy: 0.9375 - val_loss: 1.3934\nEpoch 40/100\n10/10 - 1s - 113ms/step - accuracy: 0.9704 - loss: 0.1375 - val_accuracy: 1.0000 - val_loss: 1.6130\nEpoch 41/100\n10/10 - 1s - 110ms/step - accuracy: 0.9572 - loss: 0.1405 - val_accuracy: 1.0000 - val_loss: 1.3061\nEpoch 42/100\n10/10 - 1s - 112ms/step - accuracy: 0.9638 - loss: 0.1327 - val_accuracy: 1.0000 - val_loss: 0.9775\nEpoch 43/100\n10/10 - 1s - 114ms/step - accuracy: 0.9507 - loss: 0.1585 - val_accuracy: 1.0000 - val_loss: 1.0045\nEpoch 44/100\n10/10 - 1s - 110ms/step - accuracy: 0.9441 - loss: 0.1936 - val_accuracy: 1.0000 - val_loss: 1.1656\nEpoch 45/100\n10/10 - 1s - 110ms/step - accuracy: 0.9671 - loss: 0.0900 - val_accuracy: 1.0000 - val_loss: 0.9886\nEpoch 46/100\n10/10 - 1s - 111ms/step - accuracy: 0.9605 - loss: 0.1151 - val_accuracy: 1.0000 - val_loss: 0.6414\nEpoch 47/100\n10/10 - 1s - 115ms/step - accuracy: 0.9836 - loss: 0.0554 - val_accuracy: 0.8750 - val_loss: 0.4818\nEpoch 48/100\n10/10 - 1s - 112ms/step - accuracy: 0.9737 - loss: 0.0706 - val_accuracy: 0.9375 - val_loss: 0.3388\nEpoch 49/100\n10/10 - 1s - 109ms/step - accuracy: 0.9770 - loss: 0.0982 - val_accuracy: 1.0000 - val_loss: 0.4002\nEpoch 50/100\n10/10 - 1s - 113ms/step - accuracy: 0.9803 - loss: 0.0433 - val_accuracy: 1.0000 - val_loss: 0.3426\nEpoch 51/100\n10/10 - 1s - 118ms/step - accuracy: 0.9803 - loss: 0.0521 - val_accuracy: 1.0000 - val_loss: 0.2244\nEpoch 52/100\n10/10 - 1s - 116ms/step - accuracy: 0.9737 - loss: 0.0746 - val_accuracy: 1.0000 - val_loss: 0.1702\nEpoch 53/100\n10/10 - 1s - 115ms/step - accuracy: 0.9901 - loss: 0.0494 - val_accuracy: 1.0000 - val_loss: 0.1517\nEpoch 54/100\n10/10 - 1s - 117ms/step - accuracy: 0.9901 - loss: 0.0494 - val_accuracy: 1.0000 - val_loss: 0.1012\nEpoch 55/100\n10/10 - 1s - 120ms/step - accuracy: 0.9803 - loss: 0.0634 - val_accuracy: 1.0000 - val_loss: 0.0553\nEpoch 56/100\n10/10 - 1s - 118ms/step - accuracy: 0.9704 - loss: 0.0828 - val_accuracy: 1.0000 - val_loss: 0.0694\nEpoch 57/100\n10/10 - 1s - 116ms/step - accuracy: 0.9770 - loss: 0.0874 - val_accuracy: 1.0000 - val_loss: 0.1051\nEpoch 58/100\n10/10 - 4s - 424ms/step - accuracy: 0.9671 - loss: 0.0928 - val_accuracy: 1.0000 - val_loss: 0.1174\nEpoch 59/100\n10/10 - 1s - 112ms/step - accuracy: 0.9704 - loss: 0.0693 - val_accuracy: 1.0000 - val_loss: 0.1299\nEpoch 60/100\n10/10 - 1s - 112ms/step - accuracy: 0.9836 - loss: 0.0804 - val_accuracy: 0.9375 - val_loss: 0.1798\nEpoch 61/100\n10/10 - 1s - 114ms/step - accuracy: 0.9474 - loss: 0.1542 - val_accuracy: 1.0000 - val_loss: 0.0511\nEpoch 62/100\n10/10 - 1s - 115ms/step - accuracy: 0.9572 - loss: 0.1051 - val_accuracy: 1.0000 - val_loss: 0.0241\nEpoch 63/100\n10/10 - 1s - 112ms/step - accuracy: 0.9507 - loss: 0.1264 - val_accuracy: 1.0000 - val_loss: 0.0265\nEpoch 64/100\n10/10 - 1s - 113ms/step - accuracy: 0.9507 - loss: 0.2049 - val_accuracy: 1.0000 - val_loss: 0.0351\nEpoch 65/100\n10/10 - 1s - 112ms/step - accuracy: 0.9638 - loss: 0.1301 - val_accuracy: 1.0000 - val_loss: 0.0739\nEpoch 66/100\n10/10 - 1s - 113ms/step - accuracy: 0.9868 - loss: 0.0624 - val_accuracy: 1.0000 - val_loss: 0.0344\nEpoch 67/100\n10/10 - 1s - 112ms/step - accuracy: 0.9737 - loss: 0.0675 - val_accuracy: 1.0000 - val_loss: 0.0054\nEpoch 68/100\n10/10 - 1s - 110ms/step - accuracy: 0.9671 - loss: 0.1009 - val_accuracy: 1.0000 - val_loss: 0.0057\nEpoch 69/100\n10/10 - 1s - 110ms/step - accuracy: 0.9704 - loss: 0.0923 - val_accuracy: 1.0000 - val_loss: 0.0132\nEpoch 70/100\n10/10 - 1s - 109ms/step - accuracy: 0.9770 - loss: 0.0810 - val_accuracy: 1.0000 - val_loss: 0.0541\nEpoch 71/100\n10/10 - 1s - 115ms/step - accuracy: 0.9474 - loss: 0.1472 - val_accuracy: 1.0000 - val_loss: 0.0385\nEpoch 72/100\n10/10 - 1s - 118ms/step - accuracy: 0.9770 - loss: 0.0923 - val_accuracy: 1.0000 - val_loss: 0.0138\nEpoch 73/100\n10/10 - 1s - 126ms/step - accuracy: 0.9704 - loss: 0.0910 - val_accuracy: 1.0000 - val_loss: 0.0211\nEpoch 74/100\n10/10 - 1s - 120ms/step - accuracy: 0.9704 - loss: 0.0801 - val_accuracy: 1.0000 - val_loss: 0.0029\nEpoch 75/100\n10/10 - 1s - 114ms/step - accuracy: 0.9901 - loss: 0.0472 - val_accuracy: 1.0000 - val_loss: 4.4790e-04\nEpoch 76/100\n10/10 - 1s - 109ms/step - accuracy: 0.9704 - loss: 0.0788 - val_accuracy: 1.0000 - val_loss: 0.0059\nEpoch 77/100\n10/10 - 1s - 107ms/step - accuracy: 0.9671 - loss: 0.0848 - val_accuracy: 1.0000 - val_loss: 0.0083\nEpoch 78/100\n10/10 - 1s - 109ms/step - accuracy: 0.9803 - loss: 0.0606 - val_accuracy: 1.0000 - val_loss: 6.5273e-04\nEpoch 79/100\n10/10 - 1s - 109ms/step - accuracy: 0.9671 - loss: 0.1323 - val_accuracy: 1.0000 - val_loss: 1.6888e-04\nEpoch 80/100\n10/10 - 1s - 113ms/step - accuracy: 0.9737 - loss: 0.0758 - val_accuracy: 1.0000 - val_loss: 0.0014\nEpoch 81/100\n10/10 - 1s - 109ms/step - accuracy: 0.9704 - loss: 0.1115 - val_accuracy: 1.0000 - val_loss: 0.0137\nEpoch 82/100\n10/10 - 1s - 109ms/step - accuracy: 0.9605 - loss: 0.1072 - val_accuracy: 1.0000 - val_loss: 0.0216\nEpoch 83/100\n10/10 - 1s - 110ms/step - accuracy: 0.9605 - loss: 0.1289 - val_accuracy: 1.0000 - val_loss: 0.0178\nEpoch 84/100\n10/10 - 4s - 413ms/step - accuracy: 0.9704 - loss: 0.0818 - val_accuracy: 1.0000 - val_loss: 0.0020\nEpoch 85/100\n10/10 - 1s - 111ms/step - accuracy: 0.9836 - loss: 0.0323 - val_accuracy: 1.0000 - val_loss: 0.0018\nEpoch 86/100\n10/10 - 1s - 111ms/step - accuracy: 0.9671 - loss: 0.1149 - val_accuracy: 1.0000 - val_loss: 3.7641e-04\nEpoch 87/100\n10/10 - 1s - 115ms/step - accuracy: 0.9901 - loss: 0.0515 - val_accuracy: 1.0000 - val_loss: 0.0047\nEpoch 88/100\n10/10 - 1s - 111ms/step - accuracy: 0.9737 - loss: 0.0804 - val_accuracy: 1.0000 - val_loss: 0.0039\nEpoch 89/100\n10/10 - 1s - 114ms/step - accuracy: 0.9934 - loss: 0.0373 - val_accuracy: 1.0000 - val_loss: 0.0024\n```\n:::\n:::\n\n\n## Evaluate the score\n\n::: {#b2a32883 .cell tags='[]' execution_count=18}\n``` {.python .cell-code}\nscore = cnn_model.evaluate( np.array(x_test), np.array(y_test_categorical), verbose=0)\n\nprint(f'test loss: {score[0]*100:.4f}')\nprint(f'test accuracy: {score[1]*100:.2f} %')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntest loss: 22.6138\ntest accuracy: 92.50 %\n```\n:::\n:::\n\n\n::: {#3f82897f .cell tags='[]' execution_count=19}\n``` {.python .cell-code}\n# Plot accuracy and loss curves\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){}\n:::\n:::\n\n\n## Summary\n\nTo summarize all the steps implemented to build a CNN Model for the ORL database of faces are provided in order below.\n\n1. Data Loading and Preprocessing:\n    - Images are loaded and normalized in the range [0, 1].\n    - The initial visualization ensures the images and labels are loaded correctly.\n    \n2. Training, Test, and Validation data splitting:\n    - The dataset is split into training and validation sets using `train_test_split()` from sklearn.model_selection.\n    - The split is 80% for training and 20% for test. This splitting helps in evaluating model performance while training.\n    - Another split of 5% of the data is made for validation purposes. This is because the number of images in the dataset overall is very low.\n    \n3. Data Shape Check:\n    - Reshaping is performed to ensure all images have the same dimensions `(112x92x1)` suitable for input to the CNN model.\n    - Labels are converted into categorical format using to_categorical() from keras.utils.\n    \n4. CNN Model Architecture:\n    - The defined CNN model comprises two Conv2D layers followed by MaxPooling layers.\n    - Dense layers with ReLU activations are included, along with dropout layers for regularization to prevent overfitting.\n    - Batch Normalization layers are also added for better convergence during training.\n    \n5. Model Compilation and Training:\n    - The model is compiled with 'adam' optimizer and 'categorical_crossentropy' loss.\n    - Model training (fit()) is performed using the training and validation data.\n    - Training history is stored to analyze the model's performance over epochs.\n    \n6. Model Evaluation:\n    - The trained model is evaluated on the validation set to calculate loss and accuracy.\n    - Finally, accuracy and loss curves are plotted to visualize the model's training and validation performance.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}