[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Synthetic Data Generation and Analysis",
    "section": "",
    "text": "This project demonstrates basic synthetic data generation and analysis using Python.\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic student test score data\ndef generate_student_scores(num_students=100):\n    # Simulate scores for three different subjects\n    math_scores = np.random.normal(loc=75, scale=10, size=num_students)\n    science_scores = math_scores + np.random.normal(loc=5, scale=5, size=num_students)\n    english_scores = np.random.normal(loc=70, scale=12, size=num_students)\n    \n    # Create a DataFrame\n    student_data = pd.DataFrame({\n        'Student_ID': range(1, num_students + 1),\n        'Math_Score': np.round(math_scores, 2),\n        'Science_Score': np.round(science_scores, 2),\n        'English_Score': np.round(english_scores, 2)\n    })\n    \n    return student_data\n\n# Generate the synthetic dataset\nstudents_df = generate_student_scores()\n\n# Descriptive Statistics\nprint(\"Descriptive Statistics:\")\nprint(students_df.describe())\n\n# Visualization of Score Distributions\nplt.figure(figsize=(12, 4))\n\n# Subplot 1: Histogram of Scores\nplt.subplot(1, 2, 1)\nstudents_df[['Math_Score', 'Science_Score', 'English_Score']].hist(bins=15, ax=plt.gca())\nplt.title('Distribution of Test Scores')\nplt.tight_layout()\n\n# Subplot 2: Box Plot of Scores\nplt.subplot(1, 2, 2)\nsns.boxplot(data=students_df[['Math_Score', 'Science_Score', 'English_Score']])\nplt.title('Box Plot of Test Scores')\nplt.tight_layout()\n\nplt.show()\n\n# Correlation Analysis\ncorrelation_matrix = students_df[['Math_Score', 'Science_Score', 'English_Score']].corr()\nprint(\"\\nCorrelation Matrix:\")\nprint(correlation_matrix)\n\n# Correlation Heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Heatmap of Test Scores')\nplt.tight_layout()\nplt.show()\n\n\nDescriptive Statistics:\n       Student_ID  Math_Score  Science_Score  English_Score\ncount  100.000000  100.000000     100.000000     100.000000\nmean    50.500000   73.961600      79.072900      70.780200\nstd     29.011492    9.082117       9.664472      13.011703\nmin      1.000000   48.800000      55.190000      31.100000\n25%     25.750000   68.987500      73.195000      62.135000\n50%     50.500000   73.730000      78.040000      71.170000\n75%     75.250000   79.062500      84.557500      78.457500\nmax    100.000000   93.520000     105.220000     116.230000\n\n\n/tmp/ipykernel_170946/3181019396.py:38: UserWarning:\n\nTo output multiple subplots, the figure containing the passed axes is being cleared.\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix:\n               Math_Score  Science_Score  English_Score\nMath_Score       1.000000       0.872429       0.190887\nScience_Score    0.872429       1.000000       0.161229\nEnglish_Score    0.190887       0.161229       1.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate additional insights\ndef analyze_student_scores(df):\n    # Calculate overall performance\n    df['Average_Score'] = df[['Math_Score', 'Science_Score', 'English_Score']].mean(axis=1)\n    \n    # Identify top and bottom performers\n    top_students = df.nlargest(5, 'Average_Score')\n    bottom_students = df.nsmallest(5, 'Average_Score')\n    \n    print(\"\\nTop 5 Students:\")\n    print(top_students)\n    \n    print(\"\\nBottom 5 Students:\")\n    print(bottom_students)\n    \n    return df\n\n# Perform analysis\nstudents_df = analyze_student_scores(students_df)\n\n\n\nTop 5 Students:\n    Student_ID  Math_Score  Science_Score  English_Score  Average_Score\n20          21       89.66          98.61          97.78      95.350000\n9           10       80.43          85.05         116.23      93.903333\n6            7       90.79         105.22          76.18      90.730000\n82          83       89.78          89.42          89.03      89.410000\n31          32       93.52          98.87          72.60      88.330000\n\nBottom 5 Students:\n    Student_ID  Math_Score  Science_Score  English_Score  Average_Score\n74          75       48.80          55.19          58.22      54.070000\n62          63       63.94          74.73          31.10      56.590000\n23          24       60.75          58.74          50.65      56.713333\n44          45       60.21          66.51          55.60      60.773333\n63          64       63.04          63.93          57.71      61.560000\n\n\n\n\n\n\nDemonstrates synthetic data generation techniques\nProvides statistical analysis of generated data\nCreates visualizations to explore data distributions\nShows correlation between different subject scores\n\n\n\n\n\nUses normal distribution to simulate realistic score variations\nIntroduces correlations between subject scores\nProvides multiple perspectives on the synthetic dataset"
  },
  {
    "objectID": "projects/project2/index.html#random-data-generation-and-visualization",
    "href": "projects/project2/index.html#random-data-generation-and-visualization",
    "title": "Synthetic Data Generation and Analysis",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic student test score data\ndef generate_student_scores(num_students=100):\n    # Simulate scores for three different subjects\n    math_scores = np.random.normal(loc=75, scale=10, size=num_students)\n    science_scores = math_scores + np.random.normal(loc=5, scale=5, size=num_students)\n    english_scores = np.random.normal(loc=70, scale=12, size=num_students)\n    \n    # Create a DataFrame\n    student_data = pd.DataFrame({\n        'Student_ID': range(1, num_students + 1),\n        'Math_Score': np.round(math_scores, 2),\n        'Science_Score': np.round(science_scores, 2),\n        'English_Score': np.round(english_scores, 2)\n    })\n    \n    return student_data\n\n# Generate the synthetic dataset\nstudents_df = generate_student_scores()\n\n# Descriptive Statistics\nprint(\"Descriptive Statistics:\")\nprint(students_df.describe())\n\n# Visualization of Score Distributions\nplt.figure(figsize=(12, 4))\n\n# Subplot 1: Histogram of Scores\nplt.subplot(1, 2, 1)\nstudents_df[['Math_Score', 'Science_Score', 'English_Score']].hist(bins=15, ax=plt.gca())\nplt.title('Distribution of Test Scores')\nplt.tight_layout()\n\n# Subplot 2: Box Plot of Scores\nplt.subplot(1, 2, 2)\nsns.boxplot(data=students_df[['Math_Score', 'Science_Score', 'English_Score']])\nplt.title('Box Plot of Test Scores')\nplt.tight_layout()\n\nplt.show()\n\n# Correlation Analysis\ncorrelation_matrix = students_df[['Math_Score', 'Science_Score', 'English_Score']].corr()\nprint(\"\\nCorrelation Matrix:\")\nprint(correlation_matrix)\n\n# Correlation Heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Heatmap of Test Scores')\nplt.tight_layout()\nplt.show()\n\n\nDescriptive Statistics:\n       Student_ID  Math_Score  Science_Score  English_Score\ncount  100.000000  100.000000     100.000000     100.000000\nmean    50.500000   73.961600      79.072900      70.780200\nstd     29.011492    9.082117       9.664472      13.011703\nmin      1.000000   48.800000      55.190000      31.100000\n25%     25.750000   68.987500      73.195000      62.135000\n50%     50.500000   73.730000      78.040000      71.170000\n75%     75.250000   79.062500      84.557500      78.457500\nmax    100.000000   93.520000     105.220000     116.230000\n\n\n/tmp/ipykernel_170946/3181019396.py:38: UserWarning:\n\nTo output multiple subplots, the figure containing the passed axes is being cleared.\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Matrix:\n               Math_Score  Science_Score  English_Score\nMath_Score       1.000000       0.872429       0.190887\nScience_Score    0.872429       1.000000       0.161229\nEnglish_Score    0.190887       0.161229       1.000000"
  },
  {
    "objectID": "projects/project2/index.html#score-analysis-and-insights",
    "href": "projects/project2/index.html#score-analysis-and-insights",
    "title": "Synthetic Data Generation and Analysis",
    "section": "",
    "text": "Code\n# Calculate additional insights\ndef analyze_student_scores(df):\n    # Calculate overall performance\n    df['Average_Score'] = df[['Math_Score', 'Science_Score', 'English_Score']].mean(axis=1)\n    \n    # Identify top and bottom performers\n    top_students = df.nlargest(5, 'Average_Score')\n    bottom_students = df.nsmallest(5, 'Average_Score')\n    \n    print(\"\\nTop 5 Students:\")\n    print(top_students)\n    \n    print(\"\\nBottom 5 Students:\")\n    print(bottom_students)\n    \n    return df\n\n# Perform analysis\nstudents_df = analyze_student_scores(students_df)\n\n\n\nTop 5 Students:\n    Student_ID  Math_Score  Science_Score  English_Score  Average_Score\n20          21       89.66          98.61          97.78      95.350000\n9           10       80.43          85.05         116.23      93.903333\n6            7       90.79         105.22          76.18      90.730000\n82          83       89.78          89.42          89.03      89.410000\n31          32       93.52          98.87          72.60      88.330000\n\nBottom 5 Students:\n    Student_ID  Math_Score  Science_Score  English_Score  Average_Score\n74          75       48.80          55.19          58.22      54.070000\n62          63       63.94          74.73          31.10      56.590000\n23          24       60.75          58.74          50.65      56.713333\n44          45       60.21          66.51          55.60      60.773333\n63          64       63.04          63.93          57.71      61.560000"
  },
  {
    "objectID": "projects/project2/index.html#key-insights",
    "href": "projects/project2/index.html#key-insights",
    "title": "Synthetic Data Generation and Analysis",
    "section": "",
    "text": "Demonstrates synthetic data generation techniques\nProvides statistical analysis of generated data\nCreates visualizations to explore data distributions\nShows correlation between different subject scores"
  },
  {
    "objectID": "projects/project2/index.html#methodology-notes",
    "href": "projects/project2/index.html#methodology-notes",
    "title": "Synthetic Data Generation and Analysis",
    "section": "",
    "text": "Uses normal distribution to simulate realistic score variations\nIntroduces correlations between subject scores\nProvides multiple perspectives on the synthetic dataset"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "Welcome to my blog! We’ll embark on an a journey of learning together. I’ll be using this space to write about what I’m learning, working on, and am curious about.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDigital Asset Display\n\n\n\n\n\n\nresources\n\n\n\nTesting image, videos, and terminal sessions display in posts\n\n\n\n\n\nNov 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nProgramming Syntax Comparisons and Examples\n\n\n\n\n\n\nresources\n\n\ncode\n\n\npython\n\n\n\nPost description\n\n\n\n\n\nNov 23, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nAwesome Sites! 😎\n\n\n\n\n\n\nresources\n\n\n\nEssential Resource Collections for Developers\n\n\n\n\n\nNov 5, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nAre you getting started learning Python? 🐍\n\n\n\n\n\n\nresources\n\n\ncode\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 20, 2021\n\n\n2 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "blog/awesome/index.html",
    "href": "blog/awesome/index.html",
    "title": "Awesome Sites! 😎",
    "section": "",
    "text": "As developers, we’re constantly searching for high-quality resources to enhance our skills and discover new tools. The “awesome” lists on GitHub have become a cornerstone of community-curated knowledge sharing. These collections are meticulously maintained by passionate developers worldwide, offering carefully selected resources across various technological domains.\nThe concept started with Sindre Sorhus’s original awesome list, which has since spawned thousands of specialized collections. These lists follow strict guidelines to ensure quality and relevance, making them invaluable for both beginners and experienced developers.\n\n\n\n\n\nAwesome Machine Learning\n\nA comprehensive collection of ML frameworks, libraries, and software\nCovers multiple programming languages including Python, R, Java, and more\nIncludes sections on computer vision, natural language processing, and deep learning\n\n\n\n\n\n\nAwesome Python\n\nEssential Python frameworks, libraries, and resources\nOrganized by application domain (web development, data science, testing, etc.)\nPerfect for both Python beginners and advanced developers\n\nAwesome Rust\n\nCurated resources for Rust programming language\nApplications, development tools, and learning materials\nGreat for those interested in systems programming and performance\n\n\n\n\n\n\nAwesome Embedded\n\nResources for embedded systems development\nHardware platforms, tools, and frameworks\nEssential for IoT and hardware developers\n\nAwesome Self Hosted\n\nSoftware solutions you can host on your own servers\nAlternatives to popular cloud services\nPerfect for privacy-conscious developers and organizations\n\nAwesome Quarto\n\nQuarto specific curated content!\n\n\n\n\n\n\nAwesome Static Site Generators\n\nTools for building modern static websites\nFrameworks across multiple programming languages\nIncludes JAMstack resources and deployment options\n\n\n\n\n\n\nAwesome Prompts\n\nCollection of effective prompts for AI models\nBest practices for prompt engineering\nExamples for various use cases and applications\n\n\n\n\n\n\nTo keep track of changes and new additions to these collections, I recommend using Track Awesome List. This tool helps you monitor updates to your favorite awesome lists and discover new resources regularly.\n\n\n\nThese curated collections represent countless hours of community effort to organize and validate the best resources in each field. Whether you’re exploring a new technology or deepening your expertise in a familiar domain, these awesome lists are invaluable starting points for your journey.\nRemember to contribute back to these collections if you discover valuable resources that aren’t already included. The strength of these lists comes from active community participation and sharing.\n– Last updated: November 2024-"
  },
  {
    "objectID": "blog/awesome/index.html#must-bookmark-collections",
    "href": "blog/awesome/index.html#must-bookmark-collections",
    "title": "Awesome Sites! 😎",
    "section": "",
    "text": "Awesome Machine Learning\n\nA comprehensive collection of ML frameworks, libraries, and software\nCovers multiple programming languages including Python, R, Java, and more\nIncludes sections on computer vision, natural language processing, and deep learning\n\n\n\n\n\n\nAwesome Python\n\nEssential Python frameworks, libraries, and resources\nOrganized by application domain (web development, data science, testing, etc.)\nPerfect for both Python beginners and advanced developers\n\nAwesome Rust\n\nCurated resources for Rust programming language\nApplications, development tools, and learning materials\nGreat for those interested in systems programming and performance\n\n\n\n\n\n\nAwesome Embedded\n\nResources for embedded systems development\nHardware platforms, tools, and frameworks\nEssential for IoT and hardware developers\n\nAwesome Self Hosted\n\nSoftware solutions you can host on your own servers\nAlternatives to popular cloud services\nPerfect for privacy-conscious developers and organizations\n\nAwesome Quarto\n\nQuarto specific curated content!\n\n\n\n\n\n\nAwesome Static Site Generators\n\nTools for building modern static websites\nFrameworks across multiple programming languages\nIncludes JAMstack resources and deployment options\n\n\n\n\n\n\nAwesome Prompts\n\nCollection of effective prompts for AI models\nBest practices for prompt engineering\nExamples for various use cases and applications"
  },
  {
    "objectID": "blog/awesome/index.html#staying-updated",
    "href": "blog/awesome/index.html#staying-updated",
    "title": "Awesome Sites! 😎",
    "section": "",
    "text": "To keep track of changes and new additions to these collections, I recommend using Track Awesome List. This tool helps you monitor updates to your favorite awesome lists and discover new resources regularly."
  },
  {
    "objectID": "blog/awesome/index.html#final-thoughts",
    "href": "blog/awesome/index.html#final-thoughts",
    "title": "Awesome Sites! 😎",
    "section": "",
    "text": "These curated collections represent countless hours of community effort to organize and validate the best resources in each field. Whether you’re exploring a new technology or deepening your expertise in a familiar domain, these awesome lists are invaluable starting points for your journey.\nRemember to contribute back to these collections if you discover valuable resources that aren’t already included. The strength of these lists comes from active community participation and sharing.\n– Last updated: November 2024-"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Household Energy Consumption Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic Data Generation and Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing a Flask API microservice with Kubernetes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWordPress with LAMP Stack\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chaance T. Graves",
    "section": "",
    "text": "Welcome to my website! 👋🏾\nMy name is Chaance Graves. Welcome to my website, where I share my blog on topics ... and projects that I’ve worked on or still tinkering with. I’m a creative problem solver eager to implement solutions to many issues foreseen in our modern society. In addition to that, I am also a big advocate for giving back to others who aspire to follow a path like mine.\n\n\nOn this site I keep a list of my projects, notes, my resume, as well as a technical blog.\n\n\n\n\nSystems Engineering 💡\nComputer Architecture\nArtificial Intelligence (Machine Learning)\nWireless Communications\nRenewable Energy"
  },
  {
    "objectID": "blog/post-with-code/index.html",
    "href": "blog/post-with-code/index.html",
    "title": "Are you getting started learning Python? 🐍",
    "section": "",
    "text": "Are you new to programming? Have questions about which language would be a good one to start?\nChoosing which programming language to learn is more a personal preference depending on what project and solution you’re looking to achieve. However, for a beginner, you have lots of flexibility to experiment with which one is right for you. One programming language I enjoy working with and others have found helpful is Python. I recommend installing Python 3 to have more community support to be developed and used for years to come.\nThere are likely millions of developers worldwide using Python to build projects, tools and using it to teach others the fundamentals of programming. You can do all kinds of other fantastic work in a multitude of domains.\n\nExample applications:\n\nWeb applications\nMachine Learning / Deep learning\nAutomating weekly emails\nSetting up jobs to update a spreadsheet\netc.\n\n\n\nHelpful online resources & books\nYou can find plentiful resources to teach yourself Python or pair with a small group to work on projects!\nCodeacademy: Codeacademy offers free code tutorials that are very popular for a good reason. The tutorials should give you a good idea of what it’s like to code Python with hands-on lessons to write code.\nLearn Python the Hard Way by Zed Shaw: This book is well received and aims to teach beginners how to read and write basic Python to understand other books on Python. It is high quality which I recommend regardless of where your current skill is. You can try the free sample to see if it would work for you.\nReal Python: This website offers in-depth tutorials to learn any aspect of Python you may be curious to explore. Check it out and keep a list of articles for future reference!\nAutomate the boring stuff with Python by Al Sweigart: This is an enjoyable read and practical for anyone who is currently working a tedious, routine job! He also offers a video course provided through Udemy.\nUdemy: You can find all sorts of courses and projects offered on this online education website. They range from beginner to advanced and have instructors teaching financial topics such as predicting stock prices, data science, full-stack web development, and more.\nI’m confident your programming journey will be well on its way towards success. Feel free to discuss any other valuable resources or projects you’ve discovered learning Python. 👌🏾\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/code-example-post/index.html",
    "href": "blog/code-example-post/index.html",
    "title": "Programming Syntax Comparisons and Examples",
    "section": "",
    "text": "A comparative guide for my Second Brain - Quick reference for common programming patterns across languages"
  },
  {
    "objectID": "blog/code-example-post/index.html#hello-world-examples",
    "href": "blog/code-example-post/index.html#hello-world-examples",
    "title": "Programming Syntax Comparisons and Examples",
    "section": "1. Hello World Examples",
    "text": "1. Hello World Examples\nLet’s start with the classic hello world program in each language:\n\nPython\ndef say_hello(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\nprint(say_hello(\"World\"))\n\n\nRust\nfn say_hello(name: &str) -&gt; String {\n    format!(\"Hello, {}!\", name)\n}\n\nfn main() {\n    println!(\"{}\", say_hello(\"World\"));\n}\n\n\nJavaScript\nconst sayHello = (name) =&gt; {\n    return `Hello, ${name}!`;\n};\n\nconsole.log(sayHello(\"World\"));"
  },
  {
    "objectID": "blog/code-example-post/index.html#working-with-arrayslists",
    "href": "blog/code-example-post/index.html#working-with-arrayslists",
    "title": "Programming Syntax Comparisons and Examples",
    "section": "2. Working with Arrays/Lists",
    "text": "2. Working with Arrays/Lists\nHere’s how to perform common operations on collections:\n\nPython\n# List comprehension and filtering\nnumbers = [1, 2, 3, 4, 5]\nsquares = [x**2 for x in numbers if x % 2 == 0]\nprint(squares)  # [4, 16]\n\n# Reduce operation\nfrom functools import reduce\nsum_all = reduce(lambda x, y: x + y, numbers)\nprint(sum_all)  # 15\n\n\nRust\nfn main() {\n    let numbers = vec![1, 2, 3, 4, 5];\n    \n    // Iterator and filter\n    let squares: Vec&lt;i32&gt; = numbers\n        .iter()\n        .filter(|&x| x % 2 == 0)\n        .map(|x| x * x)\n        .collect();\n    println!(\"{:?}\", squares);  // [4, 16]\n    \n    // Sum reduction\n    let sum: i32 = numbers.iter().sum();\n    println!(\"{}\", sum);  // 15\n}\n\n\nJavaScript\nconst numbers = [1, 2, 3, 4, 5];\n\n// Array methods and arrow functions\nconst squares = numbers\n    .filter(x =&gt; x % 2 === 0)\n    .map(x =&gt; x ** 2);\nconsole.log(squares);  // [4, 16]\n\n// Reduce method\nconst sum = numbers.reduce((acc, curr) =&gt; acc + curr, 0);\nconsole.log(sum);  // 15"
  },
  {
    "objectID": "blog/code-example-post/index.html#error-handling",
    "href": "blog/code-example-post/index.html#error-handling",
    "title": "Programming Syntax Comparisons and Examples",
    "section": "3. Error Handling",
    "text": "3. Error Handling\nDifferent approaches to handling errors across languages:\n\nPython\ndef divide(a: float, b: float) -&gt; float:\n    try:\n        return a / b\n    except ZeroDivisionError:\n        raise ValueError(\"Cannot divide by zero!\")\n    except TypeError:\n        raise TypeError(\"Arguments must be numbers!\")\n\n\nRust\nfn divide(a: f64, b: f64) -&gt; Result&lt;f64, String&gt; {\n    if b == 0.0 {\n        Err(String::from(\"Cannot divide by zero!\"))\n    } else {\n        Ok(a / b)\n    }\n}\n\n\nJavaScript\nfunction divide(a, b) {\n    if (typeof a !== 'number' || typeof b !== 'number') {\n        throw new TypeError('Arguments must be numbers!');\n    }\n    if (b === 0) {\n        throw new Error('Cannot divide by zero!');\n    }\n    return a / b;\n}"
  },
  {
    "objectID": "blog/code-example-post/index.html#working-with-objectsstructs",
    "href": "blog/code-example-post/index.html#working-with-objectsstructs",
    "title": "Programming Syntax Comparisons and Examples",
    "section": "4. Working with Objects/Structs",
    "text": "4. Working with Objects/Structs\nDefining and working with custom data structures:\n\nPython\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    hobbies: List[str]\n\n    def add_hobby(self, hobby: str):\n        self.hobbies.append(hobby)\n\nperson = Person(\"Alice\", 30, [\"reading\", \"hiking\"])\nperson.add_hobby(\"painting\")\n\n\nRust\nstruct Person {\n    name: String,\n    age: u32,\n    hobbies: Vec&lt;String&gt;,\n}\n\nimpl Person {\n    fn add_hobby(&mut self, hobby: String) {\n        self.hobbies.push(hobby);\n    }\n}\n\nfn main() {\n    let mut person = Person {\n        name: String::from(\"Alice\"),\n        age: 30,\n        hobbies: vec![\n            String::from(\"reading\"),\n            String::from(\"hiking\")\n        ],\n    };\n    person.add_hobby(String::from(\"painting\"));\n}\n\n\nJavaScript\nclass Person {\n    constructor(name, age, hobbies) {\n        this.name = name;\n        this.age = age;\n        this.hobbies = hobbies;\n    }\n\n    addHobby(hobby) {\n        this.hobbies.push(hobby);\n    }\n}\n\nconst person = new Person(\"Alice\", 30, [\"reading\", \"hiking\"]);\nperson.addHobby(\"painting\");"
  },
  {
    "objectID": "blog/code-example-post/index.html#key-takeaways",
    "href": "blog/code-example-post/index.html#key-takeaways",
    "title": "Programming Syntax Comparisons and Examples",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nPython excels in readability and has powerful list comprehensions\nRust provides strong safety guarantees and explicit error handling\nJavaScript offers flexible object handling and functional programming features"
  },
  {
    "objectID": "blog/code-example-post/index.html#notes-for-future-reference",
    "href": "blog/code-example-post/index.html#notes-for-future-reference",
    "title": "Programming Syntax Comparisons and Examples",
    "section": "Notes for Future Reference",
    "text": "Notes for Future Reference\n\nPython’s type hints are optional but helpful for documentation\nRust’s ownership system requires explicit handling of references\nJavaScript’s flexibility can be both a blessing and a curse\n\nRemember to check the official documentation for each language for best practices and more detailed explanations: - Python Docs - Rust Book - MDN JavaScript"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Household Energy Consumption Analysis",
    "section": "",
    "text": "This project simulates and analyzes household energy consumption across different seasons and appliance types.\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic energy consumption data\nseasons = ['Winter', 'Spring', 'Summer', 'Autumn']\nappliances = ['Heating', 'Cooling', 'Lighting', 'Appliances']\n\n# Create a dictionary to store our data\nenergy_data = {\n    'Season': [],\n    'Appliance': [],\n    'Energy_Consumption': []\n}\n\n# Simulate energy consumption with seasonal variations\nfor season in seasons:\n    for appliance in appliances:\n        # Different base consumption and variance for each appliance and season\n        if appliance == 'Heating':\n            base = 50 if season in ['Winter', 'Autumn'] else 20\n            variance = 15\n        elif appliance == 'Cooling':\n            base = 50 if season in ['Summer'] else 20\n            variance = 15\n        elif appliance == 'Lighting':\n            base = 30 if season in ['Winter'] else 20\n            variance = 10\n        else:  # General Appliances\n            base = 25\n            variance = 5\n        \n        # Generate 30 data points (representing days in a month)\n        consumption = np.random.normal(base, variance, 30)\n        \n        # Add to our data structure\n        energy_data['Season'].extend([season] * 30)\n        energy_data['Appliance'].extend([appliance] * 30)\n        energy_data['Energy_Consumption'].extend(consumption)\n\n# Convert to DataFrame\ndf = pd.DataFrame(energy_data)\n\n# Box plot of energy consumption by season and appliance\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Season', y='Energy_Consumption', hue='Appliance', data=df)\nplt.title('Energy Consumption by Season and Appliance')\nplt.xlabel('Season')\nplt.ylabel('Energy Consumption (kWh)')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Calculate summary statistics\nsummary = df.groupby(['Season', 'Appliance'])['Energy_Consumption'].agg(['mean', 'std']).round(2)\nprint(\"Energy Consumption Summary:\")\nprint(summary)\n\n\n\n\n\n\n\n\n\nEnergy Consumption Summary:\n                    mean    std\nSeason Appliance               \nAutumn Appliances  25.23   5.52\n       Cooling     20.43  15.06\n       Heating     53.42  14.89\n       Lighting    19.43  11.89\nSpring Appliances  25.24   5.19\n       Cooling     24.18  14.21\n       Heating     18.58  15.36\n       Lighting    20.66  10.53\nSummer Appliances  23.97   3.36\n       Cooling     51.61  13.79\n       Heating     17.85  17.38\n       Lighting    22.82   7.77\nWinter Appliances  24.90   4.54\n       Cooling     18.18  13.97\n       Heating     47.18  13.50\n       Lighting    30.13   9.92\n\n\n\n\n\n\n\nCode\n# Pivot table for easier comparison\npivot_table = df.pivot_table(\n    values='Energy_Consumption', \n    index='Season', \n    columns='Appliance', \n    aggfunc='mean'\n).round(2)\n\nplt.figure(figsize=(10, 6))\npivot_table.plot(kind='bar', ax=plt.gca())\nplt.title('Average Energy Consumption by Season and Appliance')\nplt.xlabel('Season')\nplt.ylabel('Average Energy Consumption (kWh)')\nplt.legend(title='Appliance', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# Print the pivot table\nprint(\"\\nAverage Energy Consumption by Season and Appliance:\")\nprint(pivot_table)\n\n\n\n\n\n\n\n\n\n\nAverage Energy Consumption by Season and Appliance:\nAppliance  Appliances  Cooling  Heating  Lighting\nSeason                                           \nAutumn          25.23    20.43    53.42     19.43\nSpring          25.24    24.18    18.58     20.66\nSummer          23.97    51.61    17.85     22.82\nWinter          24.90    18.18    47.18     30.13\n\n\n\n\n\n\nEnergy consumption varies significantly across seasons and appliances.\nHeating and cooling show the most pronounced seasonal variations.\nThe analysis provides insights into potential energy-saving strategies.\n\n\n\n\n\nFocus on heating efficiency during winter months\nImplement smart cooling strategies in summer\nConsider energy-efficient lighting solutions"
  },
  {
    "objectID": "projects/project1/index.html#data-generation-and-initial-analysis",
    "href": "projects/project1/index.html#data-generation-and-initial-analysis",
    "title": "Household Energy Consumption Analysis",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic energy consumption data\nseasons = ['Winter', 'Spring', 'Summer', 'Autumn']\nappliances = ['Heating', 'Cooling', 'Lighting', 'Appliances']\n\n# Create a dictionary to store our data\nenergy_data = {\n    'Season': [],\n    'Appliance': [],\n    'Energy_Consumption': []\n}\n\n# Simulate energy consumption with seasonal variations\nfor season in seasons:\n    for appliance in appliances:\n        # Different base consumption and variance for each appliance and season\n        if appliance == 'Heating':\n            base = 50 if season in ['Winter', 'Autumn'] else 20\n            variance = 15\n        elif appliance == 'Cooling':\n            base = 50 if season in ['Summer'] else 20\n            variance = 15\n        elif appliance == 'Lighting':\n            base = 30 if season in ['Winter'] else 20\n            variance = 10\n        else:  # General Appliances\n            base = 25\n            variance = 5\n        \n        # Generate 30 data points (representing days in a month)\n        consumption = np.random.normal(base, variance, 30)\n        \n        # Add to our data structure\n        energy_data['Season'].extend([season] * 30)\n        energy_data['Appliance'].extend([appliance] * 30)\n        energy_data['Energy_Consumption'].extend(consumption)\n\n# Convert to DataFrame\ndf = pd.DataFrame(energy_data)\n\n# Box plot of energy consumption by season and appliance\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Season', y='Energy_Consumption', hue='Appliance', data=df)\nplt.title('Energy Consumption by Season and Appliance')\nplt.xlabel('Season')\nplt.ylabel('Energy Consumption (kWh)')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Calculate summary statistics\nsummary = df.groupby(['Season', 'Appliance'])['Energy_Consumption'].agg(['mean', 'std']).round(2)\nprint(\"Energy Consumption Summary:\")\nprint(summary)\n\n\n\n\n\n\n\n\n\nEnergy Consumption Summary:\n                    mean    std\nSeason Appliance               \nAutumn Appliances  25.23   5.52\n       Cooling     20.43  15.06\n       Heating     53.42  14.89\n       Lighting    19.43  11.89\nSpring Appliances  25.24   5.19\n       Cooling     24.18  14.21\n       Heating     18.58  15.36\n       Lighting    20.66  10.53\nSummer Appliances  23.97   3.36\n       Cooling     51.61  13.79\n       Heating     17.85  17.38\n       Lighting    22.82   7.77\nWinter Appliances  24.90   4.54\n       Cooling     18.18  13.97\n       Heating     47.18  13.50\n       Lighting    30.13   9.92"
  },
  {
    "objectID": "projects/project1/index.html#detailed-energy-consumption-analysis",
    "href": "projects/project1/index.html#detailed-energy-consumption-analysis",
    "title": "Household Energy Consumption Analysis",
    "section": "",
    "text": "Code\n# Pivot table for easier comparison\npivot_table = df.pivot_table(\n    values='Energy_Consumption', \n    index='Season', \n    columns='Appliance', \n    aggfunc='mean'\n).round(2)\n\nplt.figure(figsize=(10, 6))\npivot_table.plot(kind='bar', ax=plt.gca())\nplt.title('Average Energy Consumption by Season and Appliance')\nplt.xlabel('Season')\nplt.ylabel('Average Energy Consumption (kWh)')\nplt.legend(title='Appliance', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n# Print the pivot table\nprint(\"\\nAverage Energy Consumption by Season and Appliance:\")\nprint(pivot_table)\n\n\n\n\n\n\n\n\n\n\nAverage Energy Consumption by Season and Appliance:\nAppliance  Appliances  Cooling  Heating  Lighting\nSeason                                           \nAutumn          25.23    20.43    53.42     19.43\nSpring          25.24    24.18    18.58     20.66\nSummer          23.97    51.61    17.85     22.82\nWinter          24.90    18.18    47.18     30.13"
  },
  {
    "objectID": "projects/project1/index.html#key-insights",
    "href": "projects/project1/index.html#key-insights",
    "title": "Household Energy Consumption Analysis",
    "section": "",
    "text": "Energy consumption varies significantly across seasons and appliances.\nHeating and cooling show the most pronounced seasonal variations.\nThe analysis provides insights into potential energy-saving strategies."
  },
  {
    "objectID": "projects/project1/index.html#recommendations",
    "href": "projects/project1/index.html#recommendations",
    "title": "Household Energy Consumption Analysis",
    "section": "",
    "text": "Focus on heating efficiency during winter months\nImplement smart cooling strategies in summer\nConsider energy-efficient lighting solutions"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "CC BY-NC-SA 4.0 license",
    "section": "",
    "text": "© 2024 Chaance T. Graves.\nMy content is released under the CC BY NC ND 4.0 license.\n   \nYou may share and adapt this content with appropriate credit and notation of any changes. You may not use this material for any commercial purposes.\n\n\n\n\n\n\nNote\n\n\n\nNote that my Opinions expressed are solely my own and do not express the views of my employer or any organizations I am associated with.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#welcome-to-my-website",
    "href": "index.html#welcome-to-my-website",
    "title": "Chaance T. Graves",
    "section": "",
    "text": "My name is Chaance Graves. Welcome to my website, where I share my blog on topics and projects that I’ve worked on or still tinkering with. I’m a creative problem solver eager to implement solutions to many issues foreseen in our modern society. In addition to that, I am also a big advocate for giving back to others who aspire to follow a path like mine."
  },
  {
    "objectID": "index.html#placeholder",
    "href": "index.html#placeholder",
    "title": "Chaance T. Graves",
    "section": "",
    "text": "On this site I keep a list of my projects, notes, my resume, as well as a technical blog."
  },
  {
    "objectID": "index.html#navigation",
    "href": "index.html#navigation",
    "title": "Chaance T. Graves",
    "section": "",
    "text": "On this site I keep a list of my projects, notes, my resume, as well as a technical blog."
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Chaance T. Graves",
    "section": "",
    "text": "Systems Engineering 💡\nComputer Architecture\nArtificial Intelligence (Machine Learning)\nWireless Communications\nRenewable Energy"
  },
  {
    "objectID": "blog/mtn-post-test/index.html",
    "href": "blog/mtn-post-test/index.html",
    "title": "Digital Asset Display",
    "section": "",
    "text": "Welcome to my post about this mountain pic. I’m testing the external display of my assets hosted via Cloudinary.\n\nCloudinary is a software–as-a-service (SaaS) solution for developers. It provides programmable media APIs that enable you to automate the entire lifecycle of your image and video assets."
  },
  {
    "objectID": "projects/wordpress-with-lamp-stack/index.html",
    "href": "projects/wordpress-with-lamp-stack/index.html",
    "title": "WordPress with LAMP Stack",
    "section": "",
    "text": "This Playbook will install a WordPress Content Management System (CMS) within a LAMP environment (Linux, Apache, MySQL, and PHP) on two remote servers in a private network. The LAMP versioning highlights the following for each layer:"
  },
  {
    "objectID": "projects/wordpress-with-lamp-stack/index.html#create-vagrant-private-network",
    "href": "projects/wordpress-with-lamp-stack/index.html#create-vagrant-private-network",
    "title": "WordPress with LAMP Stack",
    "section": "Create Vagrant private network",
    "text": "Create Vagrant private network\nCreating a robust sandbox environment for rapid prototyping eliminates the risk of crashing or breaking other functions when using servers or your local machine when purposed for other vital tasks. We’ll create a private network that you can install on any local device (i.e., laptop) as long as VirtualBox is available via Vagrant. The vagrantfile seen below can build test virtual machines (VM’s) for our Ansible playbook testing environment. If you prefer using a public cloud such as AWS, Azure, GCP, or Digital Ocean, the logical design is easy to follow.\n###############\n# Vagrantfile #\n###############\n\n# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n# All Vagrant configuration is done below. The \"2\" in Vagrant.configure\n# configures the configuration version (we support older styles for\n# backwards compatibility). Please don't change it unless you know what\n# you're doing.\n\nVagrant.configure(\"2\") do |config|\n  config.vm.define \"ansible-master\" do |vm1|\n    vm1.vm.box = \"bento/ubuntu-18.04\"\n    vm1.vm.hostname = \"ansible-master\"\n    vm1.vm.network \"private_network\", ip: \"10.23.45.10\"\n\n    config.vm.provider \"virtualbox\" do |vb|\n      vb.gui = false\n      vb.memory = \"4096\"\n      vb.cpus = \"2\"\n      vb.customize ['modifyvm', :id, '--cableconnected1', 'on']\n    end\n    config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL\n        echo \"Welcome to the Ubuntu Ansible network.\"\n    SHELL\n  end\n\n  config.vm.define \"ansible-node1\" do |vm2|\n    vm2.vm.box = \"bento/ubuntu-18.04\"\n    vm2.vm.hostname = \"ansible-node1\"\n    vm2.vm.network \"private_network\", ip: \"10.23.45.20\"\n\n    config.vm.provider \"virtualbox\" do |vb|\n      vb.gui = false\n      vb.memory = \"2048\"\n      vb.cpus = \"2\"\n      vb.customize ['modifyvm', :id, '--cableconnected1', 'on']\n    end\n    config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL\n        echo \"Welcome to the Ubuntu Ansible network.\"\n    SHELL\n  end\n  \n  config.vm.define \"ansible-node2\" do |vm3|\n    vm3.vm.box = \"bento/ubuntu-18.04\"\n    vm3.vm.hostname = \"ansible-node2\"\n    vm3.vm.network \"private_network\", ip: \"10.23.45.30\"\n\n    config.vm.provider \"virtualbox\" do |vb|\n      vb.gui = false\n      vb.memory = \"2048\"\n      vb.cpus = \"2\"\n      vb.customize ['modifyvm', :id, '--cableconnected1', 'on']\n    end\n    config.vm.provision \"shell\", run: \"always\", inline: &lt;&lt;-SHELL\n        echo \"Welcome to the Ubuntu Ansible network.\"\n    SHELL\n  end\nend"
  },
  {
    "objectID": "projects/wordpress-with-lamp-stack/index.html#configure-the-master-node-and-ssh-connection",
    "href": "projects/wordpress-with-lamp-stack/index.html#configure-the-master-node-and-ssh-connection",
    "title": "WordPress with LAMP Stack",
    "section": "Configure the Master node and SSH connection",
    "text": "Configure the Master node and SSH connection\nTo begin using Ansible to manage your server infrastructure, you need to install the Ansible software on the machine that will serve as the Ansible master node. First, connect via SSH to the virtual machine.\nPS C:\\\\..\\\\vagrant\\\\ubuntu_ansible&gt; vagrant ssh ansible-master\nBy default, Vagrant will be the user on the machine; however, an admin account with sudo privileges creates the specific purpose of using Ansible to talk to the other virtual machines in the network. Use the adduser command as the root user first to add a new user to your system:\nThere will be a prompt to enter a password of your choice.\nvagrant@ansible-master:~$ sudo -i\nroot@ansible-master:~# adduser admin\nUse the usermod command to add the user to the sudo group and test if the sudo commands work when logged into the user account.\nroot@ansible-master:~# usermod -aG sudo admin\nroot@ansible-master:~# su - admin\n\nadmin@ansible-master:~$ sudo apt-get update\nRun the following command to include the official project’s PPA (personal package archive) in your system’s list of sources:\nadmin@ansible-master:~$ sudo apt-add-repository ppa:ansible/ansible\nPress ENTER when prompted to accept the PPA addition.\nNext, refresh your system’s package index to be aware of the packages available in the newly included PPA and then proceed to install.\nadmin@ansible-master:~$ sudo apt-get update\nadmin@ansible-master:~$ sudo apt install ansible\n\nConfigure password-less authentication\nWe will set up password-less authentication for our admin user from master to all the managed nodes by generating a public-private key pair using ssh-keygen. We have pre-defined a blank password using -P “” This step will create private and public key pair located in the ~/.ssh directory.\nadmin@ansible-master:~$ ls -al ~/.ssh/\ntotal 20\ndrwx------ 2 admin admin 4096 Apr  3 08:03 .\ndrwxr-xr-x 6 admin admin 4096 Apr  4 22:57 ..\n-rw------- 1 admin admin 1675 Apr  3 07:49 id_rsa\n-rw-r--r-- 1 admin admin  402 Apr  3 07:49 id_rsa.pub\n-rw-r--r-- 1 admin admin  444 Apr  3 07:48 known_hosts\nWe will use ssh-copy-id to copy the keys to the remote managed server and add it to authorized_keys.\nadmin@ansible-master:~$ ssh-copy-id -i ~/.ssh/id_rsa.pub vagrant@10.23.45.20\nadmin@ansible-master:~$ ssh-copy-id -i ~/.ssh/id_rsa.pub vagrant@10.23.45.20\n\n\nSSH and Admin user setup with Setup playbook\nThe setup_ubuntu1804 folder in the Github repository runs an independent playbook that will execute an initial server setup for the managed nodes. The options stores in the vars/default.yml variable file. We define the following setting below:\n\ncreate_user: The name of the remote sudo user to create. In our case, it will be admin.\ncopy_local_key: Path to a local SSH public key that will be copied as an authorized key for the new user. By default, it copies the key from the current system user running Ansible.\nsys_packages: An array with a list of fundamental packages to be installed.\n\nRun the playbook with the following commands.\nadmin@ansible-master:~/.../setup_ubuntu1804$ ansible-playbook -i inventory -u admin playbook.yml\nOnce the Playbook completes as successful, you can test the SSH connection with the following Ansible commands for our inventory with ansible all -i inventory -m ping and ansible-inventory -i inventory --list.\nadmin@ansible-master:~/.../setup_ubuntu1804$ ansible all -i inventory -m ping\nansible-node2 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nansible-node1 | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n\nadmin@ansible-master:~/.../wordpress-lamp_ubuntu1804$ ansible-inventory -i inventory --list\n{\n    \"_meta\": {\n        \"hostvars\": {\n            \"ansible-node1\": {\n                \"ansible_host\": \"10.23.45.20\",\n                \"ansible_python_interpreter\": \"/usr/bin/python3\"\n            },\n            \"ansible-node2\": {\n                \"ansible_host\": \"10.23.45.30\",\n                \"ansible_python_interpreter\": \"/usr/bin/python3\"\n            }\n        }\n    },\n    \"all\": {\n        \"children\": [\n            \"servers\",\n            \"ungrouped\"\n        ]\n    },\n    \"servers\": {\n        \"hosts\": [\n            \"ansible-node1\",\n            \"ansible-node2\"\n        ]\n    }\n}"
  },
  {
    "objectID": "projects/wordpress-with-lamp-stack/index.html#run-the-wordpress-lamp-playbook",
    "href": "projects/wordpress-with-lamp-stack/index.html#run-the-wordpress-lamp-playbook",
    "title": "WordPress with LAMP Stack",
    "section": "Run the WordPress LAMP Playbook",
    "text": "Run the WordPress LAMP Playbook\nNavigate to the wordpress-lamp_ubuntu1804 folder and use the tree command to see the following structure:\nadmin@ansible-master:~/wordpress-ansible/wordpress-lamp_ubuntu1804$ tree\n.\n├── files\n│   ├── apache.conf.j2\n│   ├── client.my.cnf\n│   ├── client.my.cnf.j2\n│   └── wp-config.php.j2\n├── inventory\n├── playbook.yml\n├── readme.md\n└── vars\n    └── default.yml\nHere is the following description of what each of these files is:\n\nfiles/apache.conf.j2: Template file for setting up the Apache VirtualHost.\nfiles/wp-config.php.j2: Template file for setting up WordPress’s configuration file.\nfiles/client.my.cnf: The initial client.my.cnf is provided without a password and used to obtain a connection from which roots password updates to the managed nodes we want to store for MySQL database connection.\nfiles/client.my.cnf.j2: Contains the same structure as the initial client.my.cnf file but as a jinja2 Template file for better portability.\ninventory: Keeps track of which nodes and hosts will be a part of the infrastructure on which playbooks and ad-hoc commands will run.\nvars/default.yml: Variable file for customizing playbook settings.\nplaybook.yml: The playbook.yml file is where all tasks from this setup are defined. It starts by defining the group of servers that should target this setup (all). It uses become: true to describe that tasks should be executed with privilege escalation (sudo) by default. Then, it includes the vars/default.yml variable file to load configuration options.\n\nHere are the contents of each of the files respectively that should be edited:\nclient.my.cnf\n[client]\nuser=root\npassword=\nsocket=/var/run/mysqld/mysqld.sock\nclient.my.cnf.j2\n[client]\nuser=root\npassword={{ mysql_password }}\nsocket=/var/run/mysqld/mysqld.sock\ninventory\n[servers]\nansible-node1 ansible_host=10.23.45.20\nansible-node2 ansible_host=10.23.45.30\n\n[all:vars]\nansible_python_interpreter=/usr/bin/python3\nvars/default.yml\n---\n#System Settings\nphp_modules: [ 'php7.4-curl', 'php7.4-cli', 'php7.4-dev', 'php7.4-gd', 'php7.4-mbstring', 'php7.4-mcrypt', 'php7.4-json', 'php7.4-tidy', 'php7.4-opcache', 'php\n7.4-xml', 'php7.4-xmlrpc', 'php7.4-pdo', 'php7.4-soap', 'php7.4-intl', 'php7.4-zip' ]\n\n#MySQL Settings\nmysql_root_password: \"Passw0rd\"\nmysql_db: \"Wordpress_db\"\nmysql_user: \"db_user\"\nmysql_password: \"admin123!\"\n\n#HTTP Settings\nhttp_host: \"your_domain\"\nhttp_conf: \"your_domain.conf\"\nhttp_port: \"80\"\nThe following list contains a brief explanation of each of these variables if they should edit or change for any reason.\n\nphp_modules: An array containing PHP 7.4 extensions that support your WordPress setup. This list is extensive to support more features.\nmysql_root_password: The desired password for the root MySQL account.\nmysql_db: The name of the MySQL database intended for WordPress.\nmysql_user: The name of the MySQL user for WordPress.\nmysql_password: The password for the new MySQL user.\nhttp_host: Your domain name.\nhttp_conf: The name of the configuration file within Apache.\nhttp_port: HTTP port for the virtual host. By default, it is 80.\n\nOnce you’ve verified that the variables are correct, you can run the Playbook to install WordPress on the managed node(s) with the ansible-playbook command below:\nadmin@ansible-master:~/.../wordpress-lamp_ubuntu1804$ ansible-playbook -i inventory -u admin playbook.yml\nWhen the Playbook finishes all of its tasks, you should see a Play recap of all the events. “OK” means the task is done and configured correctly from the last run, or “changed” if Ansible finds the task alters the current state configured on the managed node(s).\nPLAY RECAP ****************************************************************************************************************************************************\nansible-node1              : ok=28   changed=10   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nansible-node2              : ok=28   changed=11   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\nTo see if WordPress is up and running, navigate to the managed node’s domain name or IP address. In our case, it would be the following:\n[&lt;http://10.23.45.20&gt;](&lt;http://10.23.45.20&gt;) → http://10.23.45.20/wp-admin/install.php\n[&lt;http://10.23.45.30&gt;](&lt;http://10.23.45.30&gt;) → http://10.23.45.30/wp-admin/install.php\n\n\n\nwordpress-1\n\n\nAfter selecting the language you’d like to use for your WordPress installation, you’ll be presented with a final step to set up your WordPress user and password so you can log into your control panel. Setting up the name of your site, email, and login credentials is straightforward at this stage.\n\n\n\nwordpress-2\n\n\nOnce you log in, you will be taken to the WordPress administration dashboard:\n\n\n\nwordpress-3\n\n\nSome common next steps for customizing your WordPress installation include choosing the permalinks setting for your posts (can be found in Settings &gt; Permalinks) and selecting a new theme (in Appearance &gt; Themes). Below is a snapshot of our “Hello World!” blog post.\n\n\n\nwordpress-4"
  },
  {
    "objectID": "projects/ansible-kops/index.html",
    "href": "projects/ansible-kops/index.html",
    "title": "Optimizing a Flask API microservice with Kubernetes",
    "section": "",
    "text": "A popular payment application, EasyPay where users add money to their wallet accounts, faces an issue in its payment success rate. The timeout that occurs with the connectivity of the database has been the reason for the issue.\nWhile troubleshooting, it is found that the database server has several downtime instances at irregular intervals. This situation compels the company to create its own infrastructure that runs in high-availability mode.\nGiven that online shopping experiences continue to evolve as per customer expectations, the developers are driven to make their app more reliable, fast, and secure for improving the performance of the current system."
  },
  {
    "objectID": "projects/ansible-kops/index.html#background-of-the-problem-statement",
    "href": "projects/ansible-kops/index.html#background-of-the-problem-statement",
    "title": "Optimizing a Flask API microservice with Kubernetes",
    "section": "",
    "text": "A popular payment application, EasyPay where users add money to their wallet accounts, faces an issue in its payment success rate. The timeout that occurs with the connectivity of the database has been the reason for the issue.\nWhile troubleshooting, it is found that the database server has several downtime instances at irregular intervals. This situation compels the company to create its own infrastructure that runs in high-availability mode.\nGiven that online shopping experiences continue to evolve as per customer expectations, the developers are driven to make their app more reliable, fast, and secure for improving the performance of the current system."
  },
  {
    "objectID": "projects/ansible-kops/index.html#create-ansible-host-virtual-machine",
    "href": "projects/ansible-kops/index.html#create-ansible-host-virtual-machine",
    "title": "Optimizing a Flask API microservice with Kubernetes",
    "section": "Create Ansible host Virtual Machine",
    "text": "Create Ansible host Virtual Machine\nWe will create our development environment inside a local virtual machine. The logical design is easy to follow if you prefer using a public cloud such as AWS, Azure, GCP, or Digital Ocean. The vagrantfile seen below uses an Ubuntu 20.04 base image provided by the Bento project. We’ll create a single VM on the local device (i.e., laptop) in VirtualBox via Vagrant. You can set the network as either “private” or “public” if the ports are forwarded when testing Docker images locally.\n# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n# All Vagrant configuration is done below. The \"2\" in Vagrant.configure\n# configures the configuration version (we support older styles for\n# backwards compatibility). Please don't change it unless you know what\n# you're doing.\nVagrant.configure(\"2\") do |config|\n  # The most common configuration options are documented and commented below.\n  # For a complete reference, please see the online documentation at\n  # https://docs.vagrantup.com.\n\n  # Every Vagrant development environment requires a box. You can search for\n  # boxes at https://vagrantcloud.com/search.\n  config.vm.box = \"bento/ubuntu-20.04\"\n  config.vm.hostname = \"ansible-controller\"\n  config.vm.network \"public_network\", ip: \"192.168.1.100\"\n  config.vm.network \"forwarded_port\", guest: 80, host: 8080, auto_correct: true\n  config.vm.provider \"virtualbox\" do |vb|\n  vb.customize ['modifyvm', :id, '--cableconnected1', 'on']\n  end"
  },
  {
    "objectID": "projects/ansible-kops/index.html#install-the-pre-requisites-from-the-ansible-kops-repository",
    "href": "projects/ansible-kops/index.html#install-the-pre-requisites-from-the-ansible-kops-repository",
    "title": "Optimizing a Flask API microservice with Kubernetes",
    "section": "Install the pre-requisites from the Ansible-KOPS repository",
    "text": "Install the pre-requisites from the Ansible-KOPS repository\nThe purpose of this repository is to provide a Kubernetes cluster in a Public Cloud. The deployment of the cluster is fully automated and managed by multiple tools such as Ansible or Kops. It follows the Best Practices of Docker, Kubernetes, AWS, and Ansible as much as possible.\n\nRequired Tools\n\n\nkOps - kOps is an official Kubernetes project for managing production-grade Kubernetes clusters to Amazon Web Services.\nkubectl - kubectl is a command-line tool for controlling Kubernetes clusters.\nAnsible - Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy.\nDocker - Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers.\nHelm - Helm is a tool for managing Charts. Charts are packages of pre-configured Kubernetes resources.\n\n\n\nPrerequisites\n\n\nAn AWS account\nAWS CLI v2\nAnsible\nBoto3 library\nDocker\nDocker Compose\nA registered domain\nCertbot\n\n\n\nKubernetes add-ons\n\n\nKube2IAM - kube2iam provides different AWS IAM roles for pods running on Kubernetes\nExternal-DNS - Configure external DNS servers (AWS Route53, Google CloudDNS, and others) for Kubernetes Ingresses and Services.\nIngress NGINX - Ingress-nginx is an Ingress controller for Kubernetes using NGINX as a reverse proxy and load balancer.\nCert-manager - Automatically provision and manage TLS certificates in Kubernetes.\n\n$ vagrant up && vagrant ssh\nOnce inside the virtual machine, clone down the ansible-kops repo and set up the environment with the install_prereqs.sh.\n$ git clone https://github.com/ctg123/ansible-kops.git\n$ cd ansible-kops\n$ chmod +x install_prereqs.sh\n$ ./install_prereqs.sh\nYou may need to reboot the machine for all the commands to appear. Once finished, check the following commands to verify that AWS CLI, Docker, and Ansible are correctly installed.\n$ aws --version\n\naws-cli/2.2.29 Python/3.8.8 Linux/5.4.0-58-generic exe/x86_64.ubuntu.20 prompt/off\n\n$ docker version\n\nClient: Docker Engine - Community\n Version:           20.10.8\n API version:       1.41\n Go version:        go1.16.6\n Git commit:        3967b7d\n Built:             Fri Jul 30 19:54:27 2021\n OS/Arch:           linux/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.8\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.16.6\n  Git commit:       75249d8\n  Built:            Fri Jul 30 19:52:33 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.9\n  GitCommit:        e25210fe30a0a703442421b0f60afac609f950a3\n runc:\n  Version:          1.0.1\n  GitCommit:        v1.0.1-0-g4144b63\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\n$ docker-compose version\n\ndocker-compose version 1.27.4, build 40524192\ndocker-py version: 4.3.1\nCPython version: 3.7.7\nOpenSSL version: OpenSSL 1.1.0l  10 Sep 2019\n\n$ ansible --version\n\nansible [core 2.11.3] \n  config file = None\n  configured module search path = ['/home/vagrant/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\n  ansible python module location = /home/vagrant/.local/lib/python3.8/site-packages/ansible\n  ansible collection location = /home/vagrant/.ansible/collections:/usr/share/ansible/collections\n  executable location = /home/vagrant/.local/bin/ansible\n  python version = 3.8.10 (default, Jun  2 2021, 10:49:15) [GCC 9.4.0]\n  jinja version = 3.0.1\n  libyaml = True"
  },
  {
    "objectID": "projects/ansible-kops/index.html#environment-setup",
    "href": "projects/ansible-kops/index.html#environment-setup",
    "title": "Optimizing a Flask API microservice with Kubernetes",
    "section": "Environment Setup",
    "text": "Environment Setup\nThe ansible packages for Kops & Kubectl are compatible with Linux-AMD64 and Darwin-AMD64 architectures. The following diagram is a visual representation of the infrastructure we will deploy to AWS.\n\n\n\naws-kops-ansible\n\n\n\nAWS account\nFor the Kubernetes to be fully operational, you need to create an IAM user for your AWS account with the following permissions:\n\nAmazonEC2FullAccess\nAmazonRouteS3FullAccess\nAmazonS3FullAccess\nIAMFullAccess\n\nOnce you have all the permissions, run the following command. Enter your AWS user credentials and select the region you will use in your environment.\n$ aws configure\n\n# Optionally you can specify the AWS Profile\n$ aws configure --profile &lt;profile_name&gt;\n\n# You will be prompted for your access keys\nAWS Access Key ID [None]: AKIA************\nAWS Secret Access Key [None]: kCcT****************\nDefault region name [None]: us-east-1\nDefault output format [None]: json\n\n\nRegister your Domain\n\nYou must own a registered domain to complete the Kubernetes cluster deployment by using either method(s) seen below\n\nNew Domain: Register a new domain using AWS Route53.\nExisting Domain: Create a subdomain without migrating the parent domain.\n\nCreate a hosted zone for your subdomain(example.mydomain.com.)\nTake note of your NS record.\nLog into your domain registrar account.\nCreate the corresponding NS record to let your domain know that your subdomain is hosted on AWS.\n\n\nDOMAIN                   TTL       TYPE                     TARGET\nexample.mydomain.com.      0         NS      ns-xxxx.awsdns-xx.org\nexample.mydomain.com.      0         NS      ns-yyy.awsdns-yy.org\n...\n\n\nCertbot\n\nWe’ll use the Route53 DNS plugin for Certbot. This plugin automates completing a DNS-01 challenge (DNS01) by creating and subsequently removing TXT records using the Amazon Web Services Route 53 API. My example is for my registered domain. To initiate a DNS challenge, please execute the following command:\n$ python3 -m pip install certbot-dns-route53 --user\n\n$ certbot certonly --dns-route53 -d ctgkube.com \\\n--config-dir ~/.config/letsencrypt \\\n--logs-dir /tmp/letsencrypt \\\n--work-dir /tmp/letsencrypt \\\n--dns-route53-propagation-seconds 30\n\n\nEnvironment variables\n\nYou will find all of the environment variables in the group_vars directory.\n\ngroup_vars/all.yml contains the global environment variables.\n\n#####################\n# ~~ Domain name ~~ #\ncluster_name: ctgkube.com\n\n##########################\n# ~~ Kops state store ~~ #\nbucket_name: ctgadget-kops-store\n\n###############################################\n# ~~ SSH public key to access Kubernetes API ~~#\n# Enter the full path with the name of the SSH public key created by the generate-ssh-key.yml file.\nssh_pub_key: ~/.ssh/ctgkube.pub\n\n########################################\n# ~~ AWS environment for kubernetes ~~ #\n\n# Match with your AWS profile in case of multi-account environment\naws_profile: default\naws_region: us-east-1\n\n# Be careful, master_zones must match maser_node_count\n# Example: can't have 1 master in 2 AWS availability zones\nmaster_zones: us-east-1a\naws_zones: us-east-1a,us-east-1b,us-east-1c\n\n# EC2 host sizing\n# (Ubuntu 20.04 LTS)\nbase_image: ami-019212a8baeffb0fa\n\n# Kubernetes master nodes\nmaster_instance_type: t3.medium\nmaster_node_count: 1\n\n# Kubernetes worker nodes\nworker_instance_type: t3.medium\nworker_node_count: 3\n\n############################################\n# ~~ Let's encrypt domain's email owner ~~ #\nemail_owner: chaance.graves@ctginnovations.io\n\n\nGenerate the SSH Keys and deploy the cluster\n\nWhen the environment and the pre-requisites configures, run the following playbooks with Ansible. The generate-ssh-key.yml will create the SSH key pair to access Kubernetes API, which you can use to log in to the master node with. Once generated, make sure the path matches the variable specified in the group_vars directory. You’re now ready to run the deploy-cluster.yml playbook!\n$ ansible-playbook generate-ssh-key.yml\n$ ansible-playbook deploy-cluster.yml --ask-become-pass\nIt should take approximately 8 - 10 minutes to complete.\n\n\n\nasciicast\n\n\n\n\nInstall Kubernetes Dashboard\nA critical feature for any Kubernetes cluster is efficient monitoring of all resources with an accessible UI. The Kubernetes dashboard enables the ability to deploy containerized applications, troubleshoot pods, and manage other cluster resources such as scaling a deployment, initiating rolling updates, resetting pods instead of using the kubectl command.\nRun the deploy_dashboard.sh to pull from the latest version of the Kubernetes dashboard stored in the official Github repo. You can check at this link to see which version is the latest release and update the script accordingly.\nOnce complete, the default service type configures as a ClusterIP. We will change this to LoadBalancer to access it externally. You can find the service type and edit it with the following command:\n$ kubectl -n kubernetes-dashboard edit svc kubernetes-dashboard\nMake sure the service type changed to LoadBalancer successfully. You should get an AWS ELB address as an output.\n$ kubectl -n kubernetes-dashboard get svc\nNAME                        TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)         AGE\ndashboard-metrics-scraper   ClusterIP      100.67.72.147   &lt;none&gt;                                                                    8000/TCP        5h20m\nkubernetes-dashboard        LoadBalancer   100.69.145.80   a6c1db00d3d9d42659150be7771c2ba5-1256148891.us-east-1.elb.amazonaws.com   443:31463/TCP   5h20m\nAn output of the script produced a security token needed to log in. Copy the token and enter it in the dashboard. You will then sign into the Kubernetes dashboard. You can retrieve the token when needed with this command:\n$ kubectl get secret $(kubectl get serviceaccount dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode\n\n\n\nKubernetes Dashboard Login\n\n\n\n\n\nExample Kubernetes Dashboard UI"
  },
  {
    "objectID": "projects/ansible-kops/index.html#deploy-flask-api-mongodb-app-on-kubernetes",
    "href": "projects/ansible-kops/index.html#deploy-flask-api-mongodb-app-on-kubernetes",
    "title": "Optimizing a Flask API microservice with Kubernetes",
    "section": "Deploy Flask API + MongoDB app on Kubernetes",
    "text": "Deploy Flask API + MongoDB app on Kubernetes\nWe will develop a simple Python Flask API application, which will communicate to a MongoDB database, containerize it using Docker, and deploy it to the Kubernetes cluster.\n\nPrerequisites for development on a local machine\nInstall the following python libraries using pip located in the requirements.txt file in the payment-app directory. You will have Flask running locally on your machine prior to deploying the Docker image to Kubernetes.\n$ cd ansible-kops/payment-app\n$ python3 -m pip install -r requirements.txt --user\n$ pip list\n\n\nCreating the Flask Payment application\n\nWe’ll produce a simple RESTful API to create, read, update, and delete (CRUD) payment entries. The app will store the data in a MongoDB database, an open-source database that stores flexible JSON-like documents that is Non-relational (often called NoSQL databases).\nBy default, when a MongoDB Server instance starts on a machine, it listens to port 27017. The Flask-PyMongo module helps us to bridge Flask and MongoDB and provides some convenience helpers. An objectId module is a tool for working with MongoDB ObjectId, the default value of _id field of each document, generated during the creation of any document.\nThe app.py which can run on any host (python app.py), can be accessed at http://localhost:5000/.\nfrom flask import Flask, request, jsonify\nfrom flask_pymongo import PyMongo\nfrom bson.objectid import ObjectId\nfrom flask_cors import CORS\nimport socket\n\n# Configuration\nDEBUG = True\n\n# Instantiate the app\napp = Flask(__name__)\napp.config[\"MONGO_URI\"] = \"mongodb://mongo:27017/dev\"\napp.config['JSONIFY_PRETTYPRINT_REGULAR'] = True\nmongo = PyMongo(app)\ndb = mongo.db\n\n# enable CORS\nCORS(app, resources={r'/*': {'origins': '*'}})\n\n# UI message to show which pods the Payment API container is running\n@app.route(\"/\")\ndef index():\n    hostname = socket.gethostname()\n    return jsonify(\n        message=\"Welcome to the EasyPay app. I am running inside the {} pod!\".format(hostname)\n    )\n\n@app.route(\"/payments\")\ndef get_all_payments():\n    payments = db.payment.find()\n    data = []\n    for payment in payments:\n        item = {\n            \"id\": str(payment[\"_id\"]),\n            \"payment\": payment[\"payment\"]\n        }\n        data.append(item)\n    return jsonify(\n        data=data\n    )\n\n# POST Method to collect a user's payment\n@app.route(\"/payments\", methods=[\"POST\"])\ndef add_payment():\n    data = request.get_json(force=True)\n    db.payment.insert_one({\"payment\": data[\"payment\"]})\n    return jsonify(\n        message=\"Payment saved successfully to your account!\"\n    )\n\n# PUT Method to update a user's payment\n@app.route(\"/payments/&lt;id&gt;\", methods=[\"PUT\"])\ndef update_payment(id):\n    data = request.get_json(force=True)[\"payment\"]\n    response = db.payment.update_one({\"_id\": ObjectId(id)}, {\"$set\": {\"payment\": data}})\n    if response.matched_count:\n        message = \"Payment updated successfully!\"\n    else:\n        message = \"No Payments were found!\"\n    return jsonify(\n        message=message\n    )\n\n# DELETE Method to delete a user's payment\n@app.route(\"/payments/&lt;id&gt;\", methods=[\"DELETE\"])\ndef delete_payment(id):\n    response = db.payment.delete_one({\"_id\": ObjectId(id)})\n    if response.deleted_count:\n        message = \"Payment deleted successfully!\"\n    else:\n        message = \"No Payments were found!\"\n    return jsonify(\n        message=message\n    )\n\n# POST Method to delet all payment data\n@app.route(\"/payments/delete\", methods=[\"POST\"])\ndef delete_all_payments():\n    db.payment.remove()\n    return jsonify(\n        message=\"All Payments deleted!\"\n    )\n\n# The app server will be able to run locally at port 5000\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5000)\nWe first import all the required modules and create instances of the Flask class (the app) and the PyMongo class (the database). Note that the hostname in the MONGO_URI Flask configuration variable defines the mongo instead of localhost. Mongo will be the name of our database container, and containers in the same Docker network can talk to each other by their names.\nOur app consists of six functions which are assigned URLs by @app.route() Python decorator. At first glance, it is easy to understand that the decorator is telling our app to execute the underlying function whenever a user visits our @app domain at the given route().\n\nindex() - displays a welcome message for the app. It Also displays the hostname of the machine where our app is running. This is useful to understand that we will be hitting a random pod each time we try to access our app on Kubernetes.\nget_all_payments() - displays all the payments that are available in the database as a list of dictionaries.\nadd_payment() - adds a new payment that is stored in the database with a unique ID.\nupdate_payment(id) - modifies any existing payment entry. If no payment data is found with the queried ID, the appropriate message is returned.\ndelete_payment(id) - removes that entry of the task having the queried ID from the database. Returns appropriate message if no task with the specified ID is found.\ndelete_all_payments() - removes all the payment data and returns an empty list.\n\nIn the final section, where we run the app, we define the host parameter as ‘0.0.0.0’ to make the server publicly available, running on the machine’s IP address, which will be inside a unique container.\n\n\nContainerizing the application\n\nOnce you have Docker installed locally, we will store our images to Docker Hub. Use the docker login command to authorize Docker to connect to your Docker Hub account.\nLet’s build a Docker image of the app to push to the Docker Hub registry. In the directory payment-app, a Dockerfile with the following contents to create the image:\n######################################\n# ~~ DOCKERFILE for Flask API app ~~ #\n######################################\n\nFROM python:alpine3.9\nCOPY . /app\nWORKDIR /app\nRUN pip install -r requirements.txt\nENV PORT 5000\nEXPOSE 5000\nENTRYPOINT [ \"python\" ]\nCMD [ \"app.py\" ]\nWe are using the official Python3.9 image, based on the Alpine Linux project, as the base image and copying our working directory’s contents to a new directory on the image. We are instructing the image to expose the port 5000 when run as a container, on which we can access our app. Finally, our app container configures to run python app.py automatically when deployed to a pod.\nHere, we build our image with the tag &lt;username&gt;/&lt;image-name&gt;:&lt;version&gt; format using the below command:\n$ docker build -t ctgraves16/paymentapp-python:1.0.0 .\nand then push it to the Docker Hub registry. It will be publicly available where anyone in the world can download and run it:\n$ docker push ctgraves16/paymentapp-python:1.0.0\n\n👉🏾 NOTE: Ensure to replace “ctgraves16” with your Docker Hub username.\n\nNow that we containerized the app, what about the database? How can we containerize that? We don’t have to worry about it as we can easily use the official mongo Docker image and run it on the same network as the app container.\nRun the below commands to test the image locally where it can be accessible at http://localhost:5000/\n$ docker network create payment-app-net\n$ docker run --name=mongo --rm -d --network=payment-app-net mongo\n$ docker run --name=paymentapp-python --rm -p 5000:5000 -d --network=payment-app-net ctgraves16/paymentapp-python:1.0.0\n\n\n\nPayment App on Localhost\n\n\n\n\nDeploy the application and MongoDB database to Kubernetes\n\nWe can check on how the nodes are set up by running kubectl get nodes.\n~/ansible-kops$ kubectl get nodes\nNAME                             STATUS   ROLES                  AGE   VERSION\nip-172-20-125-204.ec2.internal   Ready    node                   17h   v1.21.3\nip-172-20-42-157.ec2.internal    Ready    control-plane,master   17h   v1.21.3\nip-172-20-50-137.ec2.internal    Ready    node                   17h   v1.21.3\nip-172-20-81-89.ec2.internal     Ready    node                   17h   v1.21.3\nThe deploy.sh script automates all the steps for setting up the deployments and services. The deployment of the resources are in the following order:\n\nmongo-pv.yml Persistent Volume:\n\nThe mongo-pv.yml file creates a storage volume of 256 MB to be made available to the mongo container. The contents of this volume persist, even if the MongoDB pod is deleted or moved to a different node.\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: mongo-pv\n  labels:\n    type: local\nspec:\n  capacity:\n    storage: 256Mi\n  storageClassName: default\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /tmp/db\nWe’ll use a local path /tmp/db on the host as the disk path for simplicity.\n\n👉🏾 NOTE: Both PVC and PV must have the same class, otherwise, a PVC will not find a PV, and STATUS of such a PVC will be Pending.\n\n$ kubectl get storageclass -o wide\nNAME                      PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ndefault                   kubernetes.io/aws-ebs   Delete          Immediate              false                  17h\ngp2                       kubernetes.io/aws-ebs   Delete          Immediate              false                  17h\nkops-ssd-1-17 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   true                   17h\n\nmongo-pvc Persistent Volume Claim:\n\nThe mongo-pvc.yml file claims the storage create above and mounts onto the mongo container. Kops creates a default StorageClass set where our PVC will reside.\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mongo-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 256Mi\nThe Persistent Volume Claim will show that the volume status is now changed to Bound when the scripts are complete.\n$ kubectl get pv && kubectl get pvc\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM               STORAGECLASS    REASON   AGE\nmongo-pv                                   256Mi      RWO            Retain           Available                       default                  17h\npvc-ee7ae28d-a7ad-478e-ba5f-038aa830b1b2   1Gi        RWO            Delete           Bound       default/mongo-pvc   kops-ssd-1-17            17h\nNAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE\nmongo-pvc   Bound    pvc-ee7ae28d-a7ad-478e-ba5f-038aa830b1b2   1Gi        RWO            kops-ssd-1-17   17h\n\nmongo Deployment:\n\nThe mongo-deployment.yml file is where we define the mongo deployment that creates a single instance of a MongoDB database. Here, we’ll expose the native port 27017, which other pods can access. The persistent volume will then proceed to mount onto a directory inside the container.\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n        - name: mongo\n          image: mongo\n          ports:\n            - containerPort: 27017\n          volumeMounts:\n            - name: storage\n              mountPath: /data/db\n      volumes:\n        - name: storage\n          persistentVolumeClaim:\n            claimName: mongo-pvc\n\nmongo Service:\n\nThis service, defined by mongo-svc.yml, is set as a ClusterIP (default type of Service in Kubernetes). This service makes the mongo pod accessible from within the cluster but not from outside. The only resource that should have access to the MongoDB database is the payment app.\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongo\nspec:\n  selector:\n    app: mongo\n  ports:\n    - port: 27017\n\npayment-app Deployment:\n\nThe payment-app-deployment.yml file defines the deployment of our app running in a pod on any worker node. The spec section defines the pod where we specify the image to be pulled and run. Port 5000 of the pod is exposed.\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payment-app\n  labels:\n    app: payment-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: payment-app\n  template:\n    metadata:\n      labels:\n        app: payment-app\n    spec:\n      containers:\n        - name: payment-app\n          image: ctgraves16/paymentapp-python:1.0.0\n          ports:\n            - containerPort: 5000\n          imagePullPolicy: Always\n\npayment-app Load Balancer Service:\n\nThe LoadBalancer Service enables the pods in a deployment to be accessible from outside the cluster. Here, since we are using a custom Kubernetes cluster, we will be accessing the app from the master node at &lt;service-ip&gt;:&lt;service-port&gt;. The payment-app-svc.yml file defines this service. The advantage of using a Service is that it gives us a single consistent IP to access our app as many pods may come and go in our deployment.\napiVersion: v1\nkind: Service\nmetadata:\n  name: payment-app-svc\nspec:\n  selector:\n    app: payment-app\n  ports:\n    - port: 8080\n      targetPort: 5000\n  type: LoadBalancer\nHere, port 8080 of the service payment-app-svc is bound to port 5000 of the pods attached.\n$ kubectl get svc payment-app-svc\nNAME              TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)          AGE\npayment-app-svc   LoadBalancer   100.66.62.244   ac0a73109d80b449d8b2094246ab3e18-1598075260.us-east-1.elb.amazonaws.com   8080:30164/TCP   17h\nWe are now able to access the payment app at the ELB address:\nvagrant@ansible-controller:~/ansible-kops/kubernetes$ curl http://ac0a73109d80b449d8b2094246ab3e18-1598075260.us-east-1.elb.amazonaws.com:8080\n{\n  \"message\": \"Welcome to the EasyPay app. I am running inside the payment-app-99695c66b-dv776 pod!\"\n}\nvagrant@ansible-controller:~/ansible-kops/kubernetes$ curl http://ac0a73109d80b449d8b2094246ab3e18-1598075260.us-east-1.elb.amazonaws.com:8080\n{\n  \"message\": \"Welcome to the EasyPay app. I am running inside the payment-app-99695c66b-2bk98 pod!\"\n}\nvagrant@ansible-controller:~/ansible-kops/kubernetes$ curl http://ac0a73109d80b449d8b2094246ab3e18-1598075260.us-east-1.elb.amazonaws.com:8080\n{\n  \"message\": \"Welcome to the EasyPay app. I am running inside the payment-app-99695c66b-dv776 pod!\"\n}\nvagrant@ansible-controller:~/ansible-kops/kubernetes$ curl http://ac0a73109d80b449d8b2094246ab3e18-1598075260.us-east-1.elb.amazonaws.com:8080\n{\n  \"message\": \"Welcome to the EasyPay app. I am running inside the payment-app-99695c66b-2bk98 pod!\"\n}\nvagrant@ansible-controller:~/ansible-kops/kubernetes$ curl http://ac0a73109d80b449d8b2094246ab3e18-1598075260.us-east-1.elb.amazonaws.com:8080\n{\n  \"message\": \"Welcome to the EasyPay app. I am running inside the payment-app-99695c66b-dv776 pod!\"\n}\nvagrant@ansible-controller:~/ansible-kops/kubernetes$ curl http://ac0a73109d80b449d8b2094246ab3e18-1598075260.us-east-1.elb.amazonaws.com:8080\n{\n  \"message\": \"Welcome to the EasyPay app. I am running inside the payment-app-99695c66b-6n4kg pod!\"\n}\nWe can see that the LoadBalancer sends the traffic to any random pod each time we try to access our app.\n\npayment-app Ingress\n\nThe Ingress service allows external interfaces with the payment app using a single load balancer provided by the NGINX Ingress Controller created when Ansible deployed the Helm chart onto the cluster. We will create a domain name at easypay.ctgkube.com.\n\n\n\nPayment App Ingress\n\n\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: payment-app-ingress\n  annotations:\nspec:\n  rules:\n  - host: easypay.ctgkube.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: payment-app-svc\n            port:\n              number: 8080\n\n👉🏾 NOTE: The Ingress resource defines rules that redirect anything for easypay.ctgkube.com to payment-app-svc. Any request that doesn’t match the rule returns a 404 “Not Found” error message.\n\nIf we describe the Ingress, you’ll receive a message similar to the following:\n$ kubectl describe ingress\nName:             payment-app-ingress\nNamespace:        default\nAddress:          ab58e1ce3541e475f8365c9ddfcd4496-883895960.us-east-1.elb.amazonaws.com\nDefault backend:  default-http-backend:80 (&lt;error: endpoints \"default-http-backend\" not found&gt;)\nRules:\n  Host                 Path  Backends\n  ----                 ----  --------\n  easypay.ctgkube.com  \n                       /   payment-app-svc:8080 (100.117.243.5:5000,100.125.236.198:5000,100.125.236.199:5000)\nAnnotations:           &lt;none&gt;\nEvents:                &lt;none&gt;\nYou can test the NGINX Ingress Controller using the DNS URL of the ELB load balancer:\n$ curl -I http://ab58e1ce3xxxxxxx-8xxxx960.us-east-1.elb.amazonaws.com/\nHTTP/1.1 404 Not Found\nDate: Sat, 21 Aug 2021 21:11:28 GMT\nContent-Type: text/html\nContent-Length: 146\nConnection: keep-alive\nThe default server returns a “Not Found” page with a 404 status code for all the requests for domains where no Ingress rules are defined. Based on the prescribed rules, the Ingress Controller doesn’t divert traffic to the specified backend service unless the request matches the configuration. Because the host field configures for the Ingress object, you must supply the Host header of the request with the same hostname.\n$ curl -I -H \"Host: easypay.ctgkube.com\" http://ab58e1ce3541e475f8365c9ddfcd4496-883895960.us-east-1.elb.amazonaws.com/\nHTTP/1.1 200 OK\nDate: Sat, 21 Aug 2021 21:17:46 GMT\nContent-Type: application/json\nContent-Length: 104\nConnection: keep-alive\nAccess-Control-Allow-Origin: *\n\npayment-app Horizontal Pod Autoscaler\n\nOne of the design requirements is to enable the cluster to scale up whenever the CPU utilization exceeds 50%. An HPA resource is at the pod level, and it scales the pods in a deployment or replica set. It implements as a Kubernetes API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics) or the custom metrics API (for all other metrics).\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: payment-app-hpa\nspec:\n  maxReplicas: 10\n  minReplicas: 3\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: payment-app\n  targetCPUUtilizationPercentage: 50\nNow that the complete application setup is ready, we can interact with the Flask app at easypay.ctgkube.com.\n\n\n\npayment-app-kubernetes\n\n\nWe can try all the API methods specified in the app to interact with the data from the command line.\n$ curl http://easypay.ctgkube.com\n{\n  \"message\": \"Welcome to the EasyPay app. I am running inside the payment-app-99695c66b-dv776 pod!\"\n}\n\n$ curl http://easypay.ctgkube.com/payments\n{\n  \"data\": [ \n    {\n      \"id\": \"61207498207b153038b3b0b3\", \n      \"payment\": \"00.00\"\n    }, \n    {\n      \"id\": \"612074b5207b153038b3b0b4\", \n      \"payment\": \"500.00\"\n    }, \n    {\n      \"id\": \"612074df879dd6b7458ac593\", \n      \"payment\": \"99.99\"\n    }, \n    {\n      \"id\": \"61207582879dd6b7458ac594\", \n      \"payment\": \"199.99\"\n    }\n  ]\n}\n\n$ curl -X POST -d \"{\\\"payment\\\": \\\"19.99\\\"}\" http://easypay.ctgkube.com/payments\n{\n  \"message\": \"Payment saved successfully to your account!\"\n}\n\n$ curl -X DELETE easypay.ctgkube.com/payments/61217595207b153038b3b0b6\n{\n  \"message\": \"Payment deleted successfully!\"\n}\n\n$ curl easypay.ctgkube.com/payments\n{\n  \"data\": [\n    {\n      \"id\": \"61206c7864a10df0ec229aca\", \n      \"payment\": \"29.99\"\n    }, \n    {\n      \"id\": \"61206c8764a10df0ec229acb\", \n      \"payment\": \"2000.00\"\n    }, \n    {\n      \"id\": \"612074b5207b153038b3b0b4\", \n      \"payment\": \"500.00\"\n    }, \n    {\n      \"id\": \"61207573207b153038b3b0b5\", \n      \"payment\": \"99.99\"\n    }, \n    {\n      \"id\": \"61207582879dd6b7458ac594\", \n      \"payment\": \"199.99\"\n    }\n  ]\n}\n\n$ curl -X POST easypay.ctgkube.com/payments/delete\n{\n  \"message\": \"All Payments deleted!\"\n}\n\n$ curl easypay.ctgkube.com/payments\n{\n  \"data\": []\n}\n\n\n\neasypay.ctgkube.com-payments\n\n\n\n\n\ndefault-deployments-kubernetes-dashboard\n\n\n\n\nInstalling the Metrics server\n\nWithin the ansible-kops repository, we have the metrics server resource files stored in the kubernetes/metrics-server folder. Run the kubectl apply -f. to deploy all the resources at the same time.\n~ ansible-kops/kubernetes/metrics-server\n\n$ ls -l\ntotal 32\n-rw-rw-r-- 1 vagrant vagrant  410 Aug 19 14:44 aggregated-metrics-reader.yaml\n-rw-rw-r-- 1 vagrant vagrant  316 Aug 19 14:44 auth-delegator.yaml\n-rw-rw-r-- 1 vagrant vagrant  338 Aug 19 14:44 auth-reader.yaml\n-rw-rw-r-- 1 vagrant vagrant  307 Aug 19 14:44 metrics-apiservice.yaml\n-rw-rw-r-- 1 vagrant vagrant 1002 Aug 19 14:44 metrics-server-deployment.yaml\n-rw-rw-r-- 1 vagrant vagrant  307 Aug 19 14:44 metrics-server-service.yaml\n-rw-rw-r-- 1 vagrant vagrant  563 Aug 19 14:44 resource-reader.yaml\n-rwxrwxr-x 1 vagrant vagrant  695 Aug 19 14:44 rm-metrics-server.sh\n\n$ kubectl apply -f.\nclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created\nclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created\nrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created\nWarning: apiregistration.k8s.io/v1beta1 APIService is deprecated in v1.19+, unavailable in v1.22+; use apiregistration.k8s.io/v1 APIService\napiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created\nserviceaccount/metrics-server created\ndeployment.apps/metrics-server created\nservice/metrics-server created\nclusterrole.rbac.authorization.k8s.io/system:metrics-server created\nclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created\n\n$ kubectl get pods -n kube-system\nNAME                                                      READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-78d6f96c7b-c4pjk                  1/1     Running   0          20h\ncalico-node-4bjcm                                         1/1     Running   0          20h\ncalico-node-74m42                                         1/1     Running   0          20h\ncalico-node-gnrh2                                         1/1     Running   0          20h\ncalico-node-sbfzl                                         1/1     Running   0          20h\ncoredns-autoscaler-6f594f4c58-k8xpx                       1/1     Running   0          20h\ncoredns-f45c4bf76-m6d9g                                   1/1     Running   0          20h\ncoredns-f45c4bf76-tk77l                                   1/1     Running   0          20h\ndns-controller-64f8b56bdc-k82kx                           1/1     Running   0          20h\netcd-manager-events-ip-172-20-42-157.ec2.internal         1/1     Running   0          20h\netcd-manager-main-ip-172-20-42-157.ec2.internal           1/1     Running   0          20h\nkops-controller-8kp2r                                     1/1     Running   0          20h\nkube-apiserver-ip-172-20-42-157.ec2.internal              2/2     Running   1          20h\nkube-controller-manager-ip-172-20-42-157.ec2.internal     1/1     Running   0          20h\nkube-proxy-ip-172-20-125-204.ec2.internal                 1/1     Running   0          20h\nkube-proxy-ip-172-20-42-157.ec2.internal                  1/1     Running   0          20h\nkube-proxy-ip-172-20-50-137.ec2.internal                  1/1     Running   0          20h\nkube-proxy-ip-172-20-81-89.ec2.internal                   1/1     Running   0          20h\nkube-scheduler-ip-172-20-42-157.ec2.internal              1/1     Running   0          20h\nkube2iam-9tfsx                                            1/1     Running   0          20h\nkube2iam-m8ls4                                            1/1     Running   0          20h\nkube2iam-xnp8z                                            1/1     Running   0          20h\nmetrics-server-6fcb6cbf6f-mrsgl                           1/1     Running   0          19h\nnginx-ingress-ingress-nginx-controller-84bf68bdd7-shkj4   1/1     Running   0          20h\n\n$ kubectl get svc -n kube-system\nNAME                                               TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)                      AGE\nkube-dns                                           ClusterIP      100.64.0.10      &lt;none&gt;                                                                   53/UDP,53/TCP,9153/TCP       20h\nmetrics-server                                     ClusterIP      100.68.52.42     &lt;none&gt;                                                                   443/TCP                      19h\nnginx-ingress-ingress-nginx-controller             LoadBalancer   100.64.25.22     ab58e1ce3541e475f8365c9ddfcd4496-883895960.us-east-1.elb.amazonaws.com   80:32111/TCP,443:30841/TCP   20h\nnginx-ingress-ingress-nginx-controller-admission   ClusterIP      100.69.127.174   &lt;none&gt;                                                                   443/TCP                      20h\n\n\nTesting the payment app HPA\n\n\n\n\nkops-hpa-ca-architecture\n\n\nBefore installing and running an open-source load testing generator to test the payment-app-hpa, let’s run the kubectl describe hpa to see all the conditions affecting the HorizontalPodAutoscaler.\n$ kubectl describe hpa\nName:                                                  payment-app-hpa\nNamespace:                                             default\nLabels:                                                &lt;none&gt;\nAnnotations:                                           &lt;none&gt;\nCreationTimestamp:                                     Sat, 21 Aug 2021 02:54:24 +0000\nReference:                                             Deployment/payment-app\nMetrics:                                               ( current / target )\n  resource cpu on pods  (as a percentage of request):  1% (1m) / 50%\nMin replicas:                                          3\nMax replicas:                                          10\nDeployment pods:                                       3 current / 3 desired\nConditions:\n  Type            Status  Reason            Message\n  ----            ------  ------            -------\n  AbleToScale     True    ReadyForNewScale  recommended size matches current size\n  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)\n  ScalingLimited  True    TooFewReplicas    the desired replica count is less than the minimum replica count\nEvents:\n  Type     Reason                        Age                From                       Message\n  ----     ------                        ----               ----                       -------\n  Normal   SuccessfulRescale             22m (x2 over 19h)  horizontal-pod-autoscaler  New size: 3; reason: Current number of replicas below Spec.MinReplicas\n  Warning  FailedGetResourceMetric       21m (x4 over 22m)  horizontal-pod-autoscaler  failed to get cpu utilization: did not receive metrics for any ready pods\n  Warning  FailedComputeMetricsReplicas  21m (x4 over 22m)  horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: did not receive metrics for any ready pods"
  },
  {
    "objectID": "projects/ansible-kops/index.html#testing-the-payment-application-with-load-testing-tools",
    "href": "projects/ansible-kops/index.html#testing-the-payment-application-with-load-testing-tools",
    "title": "Optimizing a Flask API microservice with Kubernetes",
    "section": "Testing the payment application with Load testing tools",
    "text": "Testing the payment application with Load testing tools\nFor our load testing tool, we will use Apache bench. Apache bench (also called Apache benchmark) is a helpful load testing tool for websites that run on Apache webserver. It is easy to install and allows you to simulate & test different kinds of website loads to enable your website to cope with real-world situations.\nFor Ubuntu 20.04, the following command will install Apache bench:\nsudo apt-get install apache2-utils -y\nOnce installed, you can directly use it for load testing. Here’s the syntax for Apache bench.\n$ ab &lt;OPTIONS&gt; &lt;WEB_SERVER_ADDRESS&gt;/&lt;PATH&gt;\nUsing the above command, we will specify the address from the easypay-payment-ingress at easypay.ctgkube.com. We will simulate 100,000 requests with 1000 concurrent connections to see how it will scale like it would be if it were in production.\n$ kubectl get pods\nNAME                            READY   STATUS    RESTARTS   AGE\nexternal-dns-7ff5ccbb48-p8wdd   1/1     Running   0          20h\nmongo-786f4cb565-dr62t          1/1     Running   0          19h\npayment-app-767748b689-4vl9t    1/1     Running   0          40m\npayment-app-767748b689-6vbhw    1/1     Running   0          40m\npayment-app-767748b689-pkwnz    1/1     Running   0          40m\n\n$ kubectl top pods --use-protocol-buffers\nNAME                            CPU(cores)   MEMORY(bytes)   \nexternal-dns-7ff5ccbb48-p8wdd   1m           16Mi            \nmongo-786f4cb565-dr62t          9m           71Mi            \npayment-app-767748b689-4vl9t    1m           23Mi            \npayment-app-767748b689-6vbhw    1m           23Mi            \npayment-app-767748b689-pkwnz    1m           23Mi\n\n$ ab -n 100000 -c 1000 http://easypay.ctgkube.com/payments\nThis is ApacheBench, Version 2.3 &lt;$Revision: 1843412 $&gt;\nCopyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/\nLicensed to The Apache Software Foundation, http://www.apache.org/\n\nBenchmarking easypay.ctgkube.com (be patient)\nCompleted 10000 requests\nCompleted 20000 requests\nCompleted 30000 requests\nCompleted 40000 requests\nCompleted 50000 requests\nCompleted 60000 requests\nCompleted 70000 requests\nCompleted 80000 requests\nCompleted 90000 requests\nCompleted 100000 requests\nFinished 100000 requests\n\n\nServer Software:        \nServer Hostname:        easypay.ctgkube.com\nServer Port:            80\n\nDocument Path:          /payments\nDocument Length:        340 bytes\n\nConcurrency Level:      1000\nTime taken for tests:   212.537 seconds\nComplete requests:      100000\nFailed requests:        76\n   (Connect: 0, Receive: 0, Length: 76, Exceptions: 0)\nTotal transferred:      49964000 bytes\nHTML transferred:       33975520 bytes\nRequests per second:    470.51 [#/sec] (mean)\nTime per request:       2125.374 [ms] (mean)\nTime per request:       2.125 [ms] (mean, across all concurrent requests)\nTransfer rate:          229.57 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0  630 1540.3    208   65609\nProcessing:    95  897 2306.1    254   99451\nWaiting:        0  834 1647.1    248   99451\nTotal:        189 1527 2762.2    865  100599\n\nPercentage of the requests served within a certain time (ms)\n  50%    865\n  66%   1395\n  75%   1519\n  80%   1884\n  90%   3392\n  95%   4694\n  98%   7501\n  99%  10187\n 100%  100599 (longest request)\nThe auto-scaling functionality, as we can see, was successful once Apache bench completed all the requests. The kubectl get hpa -w command monitors the performance of the cluster deployments in real time. The replica set went from 3 payment-app pods running in service to 10 to handle a 500% or higher CPU utilization, which exceeds the 50% threshold. Once the load decreases, the replica set will scale back down to its orginal state of 3.\n$ kubectl get hpa -w\nNAME              REFERENCE                TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\npayment-app-hpa   Deployment/payment-app   1%/50%    3         10        3          4d4h\npayment-app-hpa   Deployment/payment-app   559%/50%   3         10        3          4d4h\npayment-app-hpa   Deployment/payment-app   559%/50%   3         10        6          4d4h\npayment-app-hpa   Deployment/payment-app   559%/50%   3         10        10         4d4h\npayment-app-hpa   Deployment/payment-app   247%/50%   3         10        10         4d4h\npayment-app-hpa   Deployment/payment-app   1%/50%     3         10        10         4d4h\npayment-app-hpa   Deployment/payment-app   1%/50%     3         10        10         4d5h\npayment-app-hpa   Deployment/payment-app   1%/50%     3         10        3          4d5h\n\n$ kubectl top pods --use-protocol-buffers\nNAME                            CPU(cores)   MEMORY(bytes)   \nexternal-dns-7ff5ccbb48-p8wdd   1m           17Mi            \nmongo-786f4cb565-dr62t          129m         97Mi            \npayment-app-767748b689-2r54p    170m         24Mi            \npayment-app-767748b689-46hzw    169m         25Mi            \npayment-app-767748b689-4vl9t    177m         31Mi            \npayment-app-767748b689-6vbhw    174m         25Mi            \npayment-app-767748b689-g6l7c    180m         25Mi            \npayment-app-767748b689-pkwnz    177m         31Mi            \npayment-app-767748b689-q6b7l    170m         24Mi            \npayment-app-767748b689-s42wl    173m         25Mi            \npayment-app-767748b689-sjwm2    170m         24Mi            \npayment-app-767748b689-xkpct    180m         25Mi\n\n$ kubectl get pods\nNAME                            READY   STATUS    RESTARTS   AGE\nexternal-dns-7ff5ccbb48-p8wdd   1/1     Running   0          20h\nmongo-786f4cb565-dr62t          1/1     Running   0          20h\npayment-app-767748b689-2r54p    1/1     Running   0          77s\npayment-app-767748b689-46hzw    1/1     Running   0          92s\npayment-app-767748b689-4vl9t    1/1     Running   0          49m\npayment-app-767748b689-6vbhw    1/1     Running   0          48m\npayment-app-767748b689-g6l7c    1/1     Running   0          92s\npayment-app-767748b689-pkwnz    1/1     Running   0          48m\npayment-app-767748b689-q6b7l    1/1     Running   0          77s\npayment-app-767748b689-s42wl    1/1     Running   0          92s\npayment-app-767748b689-sjwm2    1/1     Running   0          77s\npayment-app-767748b689-xkpct    1/1     Running   0          77s\n\n\n\nhpa-deployments-kubernetes-dashboard"
  },
  {
    "objectID": "projects/ansible-kops/index.html#conclusion",
    "href": "projects/ansible-kops/index.html#conclusion",
    "title": "Optimizing a Flask API microservice with Kubernetes",
    "section": "Conclusion",
    "text": "Conclusion\nWow, this was quite a project! If you made it this far, congratulations! You have a fully capable microservices application deployed to a Highly Available Kubernetes cluster 👏🏾\nIf you enjoyed this project or have any other suggestions, leave a comment below. Your feedback is always welcome."
  },
  {
    "objectID": "blog/mtn-post-test/index.html#testing-the-terminal-recording-on-the-website",
    "href": "blog/mtn-post-test/index.html#testing-the-terminal-recording-on-the-website",
    "title": "Digital Asset Display",
    "section": "Testing the Terminal recording on the website",
    "text": "Testing the Terminal recording on the website\nCheck out asciinema:"
  },
  {
    "objectID": "blog/mtn-post-test/index.html#testing-the-video-displays",
    "href": "blog/mtn-post-test/index.html#testing-the-video-displays",
    "title": "Digital Asset Display",
    "section": "Testing the video displays",
    "text": "Testing the video displays\nThis example video shows its display also hosted via Cloudinary.\n\n\nVideo\nSea Turtle Example Video"
  },
  {
    "objectID": "blog/mtn-post-test/index.html#testing-the-image-display",
    "href": "blog/mtn-post-test/index.html#testing-the-image-display",
    "title": "Digital Asset Display",
    "section": "",
    "text": "Welcome to my post about this mountain pic. I’m testing the external display of my assets hosted via Cloudinary.\n\nCloudinary is a software–as-a-service (SaaS) solution for developers. It provides programmable media APIs that enable you to automate the entire lifecycle of your image and video assets."
  }
]